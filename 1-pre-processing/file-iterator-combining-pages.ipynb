{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing Pipeline\n",
    "\n",
    "This notebook processes OCR JSON files from newspaper digitization projects in two stages:\n",
    "\n",
    "##### Stage 1: Text Cleaning\n",
    "- Normalizes Unicode characters (curly quotes → straight quotes)  \n",
    "- Collapses dotted abbreviations (U.C.L. → UCL)\n",
    "- Preserves original JSON structure\n",
    "- Outputs files with `-cleaned.json` suffix\n",
    "\n",
    "##### How does it work:\n",
    "- Recursively finds all `.json` files (excluding already cleaned ones)\n",
    "- Normalizes quote characters and abbreviations  \n",
    "- Preserves complete JSON structure\n",
    "- Reports processing statistics and any errors\n",
    "\n",
    "**Output**: Creates mirror directory structure under `output-pages-cleaned/` with cleaned files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Processed: 3 file(s). Errors: 0.\n",
      "Cleaned files are under: /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/1-pre-processing/output-pages-cleaned\n"
     ]
    }
   ],
   "source": [
    "# --- Clean page JSONs, save cleaned pages, then (optionally) combine per issue --- #\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "\n",
    "# Locate this notebook/script directory\n",
    "if \"__file__\" in globals():                      # when run as a .py script\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "else:                                            # when run inside Jupyter\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "# Folder layout\n",
    "INPUT_ROOT         = BASE_DIR / \"input-pages\"                       # source pages\n",
    "OUTPUT_PAGES_ROOT  = BASE_DIR / \"output-pages-cleaned\"              # cleaned pages  (<< your “pages” folder)\n",
    "OUTPUT_ISSUES_ROOT = BASE_DIR / \"output-issues-combined-cleaned\"    # combined issues\n",
    "\n",
    "# Ensure output roots exist\n",
    "OUTPUT_PAGES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_ISSUES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Mirror the input folder tree under OUTPUT_PAGES_ROOT\n",
    "for d in [INPUT_ROOT] + [p for p in INPUT_ROOT.rglob(\"*\") if p.is_dir()]:\n",
    "    (OUTPUT_PAGES_ROOT / d.relative_to(INPUT_ROOT)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Text cleaning helpers ----------------------------------------------------\n",
    "def normalize_quotes(s: str) -> str:\n",
    "    # Curly quotes → straight quotes\n",
    "    return (\n",
    "        s.replace(\"\\u2018\", \"'\")\n",
    "         .replace(\"\\u2019\", \"'\")\n",
    "         .replace(\"\\u201C\", '\"')\n",
    "         .replace(\"\\u201D\", '\"')\n",
    "    )\n",
    "\n",
    "# Collapse dotted abbreviations like U.C.L., U.S.A., incl. spaced variants U. S. A. → UCL/USA\n",
    "_abbr_with_space = re.compile(r'\\b(?:[A-Za-z]\\s*\\.){1,}[A-Za-z]\\s*\\.?')\n",
    "_abbr_no_space   = re.compile(r'\\b(?:[A-Za-z]\\.){1,}[A-Za-z]\\.?')\n",
    "def collapse_dotted_abbreviations(s: str) -> str:\n",
    "    def repl(m):  # keep letters only\n",
    "        return ''.join(ch for ch in m.group(0) if ch.isalpha())\n",
    "    s = _abbr_with_space.sub(repl, s)\n",
    "    s = _abbr_no_space.sub(repl, s)\n",
    "    return s\n",
    "\n",
    "def clean_string(s: str) -> str:\n",
    "    s = normalize_quotes(s)\n",
    "    s = collapse_dotted_abbreviations(s)\n",
    "    return s\n",
    "\n",
    "def clean_json(obj):\n",
    "    \"\"\"Recursively clean ALL string fields while preserving structure.\"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        return clean_string(obj)\n",
    "    if isinstance(obj, list):\n",
    "        return [clean_json(v) for v in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# --- Start the process of Cleaning JSON files for all the pages ------------------------------------------------------------\n",
    "processed, skipped, errors = 0, 0, []\n",
    "\n",
    "for f in INPUT_ROOT.rglob(\"*.json\"):\n",
    "    # Skip any files that look already cleaned\n",
    "    if f.name.endswith(\"-cleaned.json\"):\n",
    "        continue\n",
    "    rel = f.relative_to(INPUT_ROOT)\n",
    "    out_dir = OUTPUT_PAGES_ROOT / rel.parent\n",
    "    out_path = out_dir / (f.stem + \"-cleaned.json\")\n",
    "    try:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "    except Exception as e:\n",
    "        errors.append((str(f), f\"read: {e}\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cleaned = clean_json(data)\n",
    "    except Exception as e:\n",
    "        errors.append((str(f), f\"clean: {e}\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as oh:\n",
    "            json.dump(cleaned, oh, ensure_ascii=False, indent=2)\n",
    "        processed += 1\n",
    "    except Exception as e:\n",
    "        errors.append((str(out_path), f\"write: {e}\"))\n",
    "\n",
    "print(f\"Done. Processed: {processed} file(s). Errors: {len(errors)}.\")\n",
    "if errors:\n",
    "    for path, msg in errors[:10]:  # show up to first 10 errors\n",
    "        print(f\"- {path} -> {msg}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"... and {len(errors)-10} more\")\n",
    "print(f\"Cleaned files are under: {OUTPUT_PAGES_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2: Page Combination  \n",
    "\n",
    "- Groups pages by issue using filename patterns (`base_001.json`, `base_002.json` → `base.json`)\n",
    "- Adds sequential article IDs and page numbers\n",
    "- Creates consolidated JSON files with consistent structure\n",
    "- Maintains directory hierarchy\n",
    "\n",
    "\n",
    "##### How does it work:\n",
    "**Input Pattern**: Files like `pi_vol_7_5_1951_001-cleaned.json`, `pi_vol_7_5_1951_002-cleaned.json`  \n",
    "**Output**: Single file `pi_vol_7_5_1951-cleaned.json` containing all articles with:\n",
    "- Sequential article IDs (1, 2, 3...)\n",
    "- Source page numbers for each article\n",
    "- Consistent `{\"articles\": [...]}` structure\n",
    "\n",
    "**Processing Summary**: Shows pages combined, article counts, and output paths for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INPUT ROOT]  /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/1-pre-processing/output-pages-cleaned\n",
      "[OUTPUT ROOT] /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/1-pre-processing/output-issues-combined-cleaned\n",
      "[WRITE] Pi-Newspaper-1978-cleaned.json  pages=[1, 2]  mode=json  articles=6\n",
      "[WRITE] Pi-Newspaper-1979-cleaned.json  pages=[1]  mode=json  articles=3\n",
      "Done. Wrote 2 file(s).\n"
     ]
    }
   ],
   "source": [
    "# Combine multi-page OCR JSONs into one file per issue.\n",
    "# Jupyter-friendly: hard-coded paths, no CLI args.\n",
    "\n",
    "import os, re, json\n",
    "from json import JSONDecodeError\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Union, Any\n",
    "\n",
    "# ---- EDIT THESE TWO LINES IF YOUR PATHS CHANGE ----\n",
    "INPUT_ROOT  = OUTPUT_PAGES_ROOT\n",
    "OUTPUT_ROOT = OUTPUT_ISSUES_ROOT\n",
    "# ---------------------------------------------------\n",
    "\n",
    "OVERWRITE = False      # set True to overwrite existing combined files\n",
    "DRY_RUN   = False      # set True to preview without writing\n",
    "\n",
    "# Matches multi-page files like <base>_001-cleaned.json\n",
    "PAGE_FILE_RE    = re.compile(r\"^(?P<base>.+?)_(?P<page>\\d{3})-cleaned\\.json$\")\n",
    "# Matches single-page cleaned file like <base>-cleaned.json (no _NNN)\n",
    "SINGLE_FILE_RE  = re.compile(r\"^(?P<base>.+?)-cleaned\\.json$\")\n",
    "\n",
    "def assert_paths():\n",
    "    if not os.path.isdir(INPUT_ROOT):\n",
    "        raise FileNotFoundError(f\"INPUT_ROOT not found: {INPUT_ROOT}\")\n",
    "    # Prevent accidental recursion if OUTPUT_ROOT is inside INPUT_ROOT\n",
    "    in_abs  = os.path.abspath(INPUT_ROOT)\n",
    "    out_abs = os.path.abspath(OUTPUT_ROOT)\n",
    "    if out_abs.startswith(in_abs + os.sep):\n",
    "        raise ValueError(\"OUTPUT_ROOT must not be inside INPUT_ROOT. Choose a sibling or separate folder.\")\n",
    "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "def mirror_dir_structure(in_root: str, out_root: str, dry_run: bool = False):\n",
    "    for dirpath, dirnames, _ in os.walk(in_root):\n",
    "        rel = os.path.relpath(dirpath, start=in_root)\n",
    "        target_dir = os.path.join(out_root, rel) if rel != os.curdir else out_root\n",
    "        if not dry_run:\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "def find_issue_groups(root: str):\n",
    "    \"\"\"\n",
    "    Return mapping: (dirpath, base) -> list of (page_num, abs_path, dirpath, filename, base)\n",
    "\n",
    "    Supports two patterns:\n",
    "      • Multi-page: <base>_NNN-cleaned.json (NNN = 001, 002, …)\n",
    "      • Single-page: <base>-cleaned.json  (treated as page 1)\n",
    "\n",
    "    If BOTH exist for the same (dirpath, base), multi-page wins.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    # Track which (dirpath, base) have multi-page matches to prioritize them\n",
    "    has_multi = set()\n",
    "\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        # First pass: collect multi-page files\n",
    "        for fn in files:\n",
    "            if not fn.endswith(\"-cleaned.json\"):\n",
    "                continue\n",
    "            m = PAGE_FILE_RE.match(fn)\n",
    "            if not m:\n",
    "                continue\n",
    "            base = m.group(\"base\")\n",
    "            page = int(m.group(\"page\"))\n",
    "            abs_path = os.path.join(dirpath, fn)\n",
    "            key = (dirpath, base)\n",
    "            has_multi.add(key)\n",
    "            groups[key].append((page, abs_path, dirpath, fn, base))\n",
    "\n",
    "        # Second pass: collect single-page files ONLY if no multi-page for same base\n",
    "        for fn in files:\n",
    "            if not fn.endswith(\"-cleaned.json\"):\n",
    "                continue\n",
    "            # ignore multipage names here\n",
    "            if PAGE_FILE_RE.match(fn):\n",
    "                continue\n",
    "            m = SINGLE_FILE_RE.match(fn)\n",
    "            if not m:\n",
    "                continue\n",
    "            base = m.group(\"base\")\n",
    "            key = (dirpath, base)\n",
    "            if key in has_multi:\n",
    "                # prefer the multi-page set; skip singleton\n",
    "                continue\n",
    "            abs_path = os.path.join(dirpath, fn)\n",
    "            # Treat as a single page = 1\n",
    "            groups[key].append((1, abs_path, dirpath, fn, base))\n",
    "\n",
    "    # Sort each group's entries by page number\n",
    "    for k in groups:\n",
    "        groups[k].sort(key=lambda x: x[0])\n",
    "    return groups\n",
    "\n",
    "def load_json_or_text(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        try:\n",
    "            return json.load(fh), \"json\"\n",
    "        except JSONDecodeError:\n",
    "            fh.seek(0)\n",
    "            return fh.read(), \"text\"\n",
    "\n",
    "def _extract_articles(data: Any) -> list:\n",
    "    \"\"\"\n",
    "    Normalize a single page payload into a list of articles:\n",
    "      - {\"articles\": [...]} -> [...]\n",
    "      - [...]                -> [...]\n",
    "      - {...}                -> [ {...} ]\n",
    "      - other                -> []\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if \"articles\" in data:\n",
    "            arts = data[\"articles\"]\n",
    "            return arts if isinstance(arts, list) else [arts]\n",
    "        return [data]\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    return []\n",
    "\n",
    "def combine_pages(paged_files: List[Tuple[int, str]]) -> Tuple[Union[dict, str], str]:\n",
    "    \"\"\"\n",
    "    Returns (combined, mode)\n",
    "      - mode == 'json'  -> {\"articles\": [...]} where each article has 'id' and 'page'\n",
    "      - mode == 'text'  -> newline-delimited fallback when any page is non-JSON\n",
    "    \"\"\"\n",
    "    parsed = []\n",
    "    all_json = True\n",
    "    for page, path in paged_files:\n",
    "        data, kind = load_json_or_text(path)\n",
    "        parsed.append((page, data, kind))\n",
    "        if kind != \"json\":\n",
    "            all_json = False\n",
    "\n",
    "    if not all_json:\n",
    "        # Fallback: newline-delimited serialization (no normalization possible)\n",
    "        lines = []\n",
    "        for page, data, kind in parsed:\n",
    "            if kind == \"json\":\n",
    "                lines.append(json.dumps(data, ensure_ascii=False))\n",
    "            else:\n",
    "                lines.append(str(data))\n",
    "        return \"\\n\".join(lines), \"text\"\n",
    "\n",
    "    # All JSON: normalize, annotate with page, then assign sequential ids\n",
    "    combined_articles = []\n",
    "    for page, data, _ in parsed:\n",
    "        for art in _extract_articles(data):\n",
    "            if not isinstance(art, dict):\n",
    "                art = {\"_raw\": art}\n",
    "            art = dict(art)\n",
    "            art[\"page\"] = page\n",
    "            combined_articles.append(art)\n",
    "\n",
    "    for i, art in enumerate(combined_articles, start=1):\n",
    "        art[\"id\"] = i\n",
    "\n",
    "    return {\"articles\": combined_articles}, \"json\"\n",
    "\n",
    "def ensure_out_dir(in_root: str, dirpath: str, out_root: str) -> str:\n",
    "    rel = os.path.relpath(dirpath, start=in_root)\n",
    "    target_dir = os.path.join(out_root, rel) if rel != os.curdir else out_root\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    return target_dir\n",
    "\n",
    "# ---- run ----\n",
    "print(f\"[INPUT ROOT]  {INPUT_ROOT}\")\n",
    "print(f\"[OUTPUT ROOT] {OUTPUT_ROOT}\")\n",
    "\n",
    "assert_paths()\n",
    "mirror_dir_structure(INPUT_ROOT, OUTPUT_ROOT, dry_run=DRY_RUN)\n",
    "\n",
    "groups = find_issue_groups(INPUT_ROOT)\n",
    "if not groups:\n",
    "    print(\"No page groups found under INPUT_ROOT.\")\n",
    "else:\n",
    "    total_written = 0\n",
    "    for (dirpath, base), entries in groups.items():\n",
    "        pages = [e[0] for e in entries]\n",
    "        file_paths = [e[1] for e in entries]\n",
    "        out_dir = ensure_out_dir(INPUT_ROOT, dirpath, OUTPUT_ROOT)\n",
    "        out_filename = f\"{base}-cleaned.json\"  # remove _NNN (or same name for the singleton)\n",
    "        out_path = os.path.join(out_dir, out_filename)\n",
    "\n",
    "        if os.path.exists(out_path) and not OVERWRITE:\n",
    "            print(f\"[SKIP] exists: {os.path.relpath(out_path, start=OUTPUT_ROOT)}\")\n",
    "            continue\n",
    "\n",
    "        paged_files = list(zip(pages, file_paths))  # (page, path)\n",
    "        combined, mode = combine_pages(paged_files)\n",
    "\n",
    "        rel_out = os.path.relpath(out_path, start=OUTPUT_ROOT)\n",
    "        print(f\"[WRITE] {rel_out}  pages={pages}  mode={mode}  articles={len(combined['articles']) if mode=='json' else 'N/A'}\")\n",
    "\n",
    "        if not DRY_RUN:\n",
    "            if mode == \"json\":\n",
    "                with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    json.dump(combined, fh, ensure_ascii=False, indent=2)\n",
    "                    fh.write(\"\\n\")\n",
    "            else:\n",
    "                with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(combined)\n",
    "        total_written += 1\n",
    "\n",
    "    print(f\"Done. {'Would write' if DRY_RUN else 'Wrote'} {total_written} file(s).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
