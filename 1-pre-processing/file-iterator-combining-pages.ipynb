{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation for the First Cell\n",
    "\n",
    "The first cell in this Jupyter Notebook performs the following tasks:\n",
    "\n",
    "1. **Imports Required Modules**:\n",
    "    - `Path` from `pathlib` for handling file paths.\n",
    "    - `re` for working with regular expressions.\n",
    "    - `defaultdict` from `collections` for grouping data.\n",
    "    - `pandas` as `pd` for data manipulation.\n",
    "\n",
    "2. **Configures Paths**:\n",
    "    - Defines `BASE_DIR` as the root directory for input files.\n",
    "    - Defines `OUTPUT_ROOT` as the directory where results will be stored, mirroring the structure of `BASE_DIR`.\n",
    "\n",
    "3. **Mirrors Folder Structure**:\n",
    "    - Creates the `OUTPUT_ROOT` directory if it doesn't exist.\n",
    "    - Recursively mirrors the folder structure of `BASE_DIR` into `OUTPUT_ROOT`.\n",
    "\n",
    "4. **Groups JSON Files by Issues**:\n",
    "    - Uses a regular expression to match filenames in the format `<issue>_<page>.json`.\n",
    "    - Groups files by their issue ID and relative directory using a `defaultdict`.\n",
    "\n",
    "5. **Sorts and Processes Files**:\n",
    "    - Defines a helper function `page_num` to extract page numbers from filenames.\n",
    "    - Sorts files within each group by page number.\n",
    "    - Constructs a list of records containing metadata for each issue, including:\n",
    "        - `relative_dir`: The relative directory of the files.\n",
    "        - `issue_id`: The unique identifier for the issue.\n",
    "        - `output_path`: The path where the consolidated JSON will be saved.\n",
    "        - `num_pages`: The number of pages in the issue.\n",
    "        - `source_files`: A list of source file paths.\n",
    "\n",
    "6. **Creates a DataFrame**:\n",
    "    - Converts the list of records into a pandas DataFrame `df`.\n",
    "    - Sorts the DataFrame by `relative_dir` and `issue_id`.\n",
    "\n",
    "7. **Displays or Prints the DataFrame**:\n",
    "    - Includes commented-out code to display or print the DataFrame and a summary of the results.\n",
    "    - Provides a summary of the number of consolidated issues and any skipped files that did not match the expected filename pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data of the given JSON files in each issue by removing unnecessary characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Processed: 373 file(s). Errors: 0.\n",
      "Cleaned files are under: /Users/stepanyan/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Knowledge Exchange and Innovation - UCL - Wikimedia - Shared working space/NER/Newswire - Jupyter/Pi-OCR-Articles-Cleaned\n"
     ]
    }
   ],
   "source": [
    "# --- Clean each page JSON and save as \"*-cleaned.json\" ------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 0) Configure paths (reuse your BASE_DIR; fallback to your full path if missing)\n",
    "try:\n",
    "    BASE_DIR\n",
    "except NameError:\n",
    "    BASE_DIR = Path(\n",
    "        \"/Users/stepanyan/Library/CloudStorage/OneDrive-UniversityCollegeLondon/\"\n",
    "        \"Knowledge Exchange and Innovation - UCL - Wikimedia - Shared working space/\"\n",
    "        \"NER/Newswire - Jupyter/Pi-OCR-Articles\"\n",
    "    )\n",
    "BASE_DIR = Path(BASE_DIR)\n",
    "OUTPUT_ROOT = BASE_DIR.parent / \"Pi-OCR-Articles-Cleaned\"  # <- change to BASE_DIR if you want to save alongside originals\n",
    "\n",
    "# Make sure output root and mirror subfolders exist\n",
    "OUTPUT_ROOT.mkdir(exist_ok=True)\n",
    "for d in [BASE_DIR] + [p for p in BASE_DIR.rglob(\"*\") if p.is_dir()]:\n",
    "    (OUTPUT_ROOT / d.relative_to(BASE_DIR)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Text cleaning helpers ----------------------------------------------------\n",
    "def normalize_quotes(s: str) -> str:\n",
    "    # Curly quotes → straight quotes\n",
    "    return (\n",
    "        s.replace(\"\\u2018\", \"'\")\n",
    "         .replace(\"\\u2019\", \"'\")\n",
    "         .replace(\"\\u201C\", '\"')\n",
    "         .replace(\"\\u201D\", '\"')\n",
    "    )\n",
    "\n",
    "# Collapse dotted abbreviations like U.C.L., U.S.A., incl. spaced variants U. S. A. → UCL/USA\n",
    "_abbr_with_space = re.compile(r'\\b(?:[A-Za-z]\\s*\\.){1,}[A-Za-z]\\s*\\.?')\n",
    "_abbr_no_space   = re.compile(r'\\b(?:[A-Za-z]\\.){1,}[A-Za-z]\\.?')\n",
    "def collapse_dotted_abbreviations(s: str) -> str:\n",
    "    def repl(m):  # keep letters only\n",
    "        return ''.join(ch for ch in m.group(0) if ch.isalpha())\n",
    "    s = _abbr_with_space.sub(repl, s)\n",
    "    s = _abbr_no_space.sub(repl, s)\n",
    "    return s\n",
    "\n",
    "def clean_string(s: str) -> str:\n",
    "    s = normalize_quotes(s)\n",
    "    s = collapse_dotted_abbreviations(s)\n",
    "    return s\n",
    "\n",
    "def clean_json(obj):\n",
    "    \"\"\"Recursively clean ALL string fields while preserving structure.\"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        return clean_string(obj)\n",
    "    if isinstance(obj, list):\n",
    "        return [clean_json(v) for v in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# --- Process files ------------------------------------------------------------\n",
    "processed, skipped, errors = 0, 0, []\n",
    "\n",
    "for f in BASE_DIR.rglob(\"*.json\"):\n",
    "    # Skip any files that look already cleaned\n",
    "    if f.name.endswith(\"-cleaned.json\"):\n",
    "        continue\n",
    "    rel = f.relative_to(BASE_DIR)\n",
    "    out_dir = OUTPUT_ROOT / rel.parent\n",
    "    out_path = out_dir / (f.stem + \"-cleaned.json\")\n",
    "    try:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "    except Exception as e:\n",
    "        errors.append((str(f), f\"read: {e}\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cleaned = clean_json(data)\n",
    "    except Exception as e:\n",
    "        errors.append((str(f), f\"clean: {e}\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as oh:\n",
    "            json.dump(cleaned, oh, ensure_ascii=False, indent=2)\n",
    "        processed += 1\n",
    "    except Exception as e:\n",
    "        errors.append((str(out_path), f\"write: {e}\"))\n",
    "\n",
    "print(f\"Done. Processed: {processed} file(s). Errors: {len(errors)}.\")\n",
    "if errors:\n",
    "    for path, msg in errors[:10]:  # show up to first 10 errors\n",
    "        print(f\"- {path} -> {msg}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"... and {len(errors)-10} more\")\n",
    "print(f\"Cleaned files are under: {OUTPUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining individual Pages of an issue into a single JSON\n",
    "\n",
    "The following code combines individual pages of a single issue, which were cleaned above into a single JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INPUT ROOT]  /Users/stepanyan/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Knowledge Exchange and Innovation - UCL - Wikimedia - Shared working space/NER/Newswire - Jupyter/Pi-OCR-Articles-Cleaned\n",
      "[OUTPUT ROOT] /Users/stepanyan/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Knowledge Exchange and Innovation - UCL - Wikimedia - Shared working space/NER/Newswire - Jupyter/Pi-OCR-Articles-Cleaned-Combined\n",
      "[WRITE] 1951/pi_vol_8_2_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=41\n",
      "[WRITE] 1951/pi_vol_7_5_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=32\n",
      "[WRITE] 1951/pi_vol_7_7_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=32\n",
      "[WRITE] 1951/pi_vol_9_4_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=30\n",
      "[WRITE] 1951/pi_vol_9_3_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=36\n",
      "[WRITE] 1951/pi_vol_7_8_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=24\n",
      "[WRITE] 1951/pi_vol_7_9_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=32\n",
      "[WRITE] 1951/pi_vol_8_1_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=33\n",
      "[WRITE] 1951/pi_vol_7_6_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=21\n",
      "[WRITE] 1951/pi_vol_7_4_1951-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=29\n",
      "[WRITE] 1950/pi_vol_5_10_1950-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=36\n",
      "[WRITE] 1950/pi_vol_6_2_1950-cleaned.json  pages=[1, 2, 3, 4, 5, 6, 7, 8]  mode=json  articles=56\n",
      "[WRITE] 1950/pi_vol_7_1_1950-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=29\n",
      "[WRITE] 1950/pi_vol_7_3_1950-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=37\n",
      "[WRITE] 1950/pi_vol_6_4_1950-cleaned.json  pages=[1, 2, 3, 4, 6, 7, 8]  mode=json  articles=26\n",
      "[WRITE] 1950/pi_rag_1950-cleaned.json  pages=[1, 2]  mode=json  articles=13\n",
      "[WRITE] 1950/pi_vol_6_1_1950-cleaned.json  pages=[1, 2, 3, 4, 5, 6, 7, 8]  mode=json  articles=48\n",
      "[WRITE] 1950/pi_vol_7_2_1950-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=34\n",
      "[WRITE] 1950/pi_vol_6_3_1950-cleaned.json  pages=[1, 2, 3, 4, 5, 6]  mode=json  articles=37\n",
      "[WRITE] 1950/pi_vol_5_11_1950-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=38\n",
      "[WRITE] 1947/pi_emergency_vol_1_2_1947-cleaned.json  pages=[1, 2, 3, 4, 5, 6, 7]  mode=json  articles=21\n",
      "[WRITE] 1947/pi_14_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=41\n",
      "[WRITE] 1947/pi_vol_2_7_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=37\n",
      "[WRITE] 1947/pi_13_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=32\n",
      "[WRITE] 1947/pi_vol_2_8_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=24\n",
      "[WRITE] 1947/pi_11_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=43\n",
      "[WRITE] 1947/pi_10_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=33\n",
      "[WRITE] 1947/pi_12_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=27\n",
      "[WRITE] 1947/pi_emergency_vol_1_1_1947-cleaned.json  pages=[1, 3, 4, 5, 6, 7, 8]  mode=json  articles=10\n",
      "[WRITE] 1947/pi_vol_2_9_1947-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=25\n",
      "[WRITE] 1949/pi_vol_4_4_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=43\n",
      "[WRITE] 1949/pi_vol_5_7_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=47\n",
      "[WRITE] 1949/pi_vol_5_5_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=31\n",
      "[WRITE] 1949/pi_vol_4_3_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=50\n",
      "[WRITE] 1949/pi_vol_5_8_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=31\n",
      "[WRITE] 1949/pi_vol_4_1_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=40\n",
      "[WRITE] 1949/pi_vol_5_2_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=41\n",
      "[WRITE] 1949/pi_vol_5_3_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=29\n",
      "[WRITE] 1949/pi_vol_4_2_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=51\n",
      "[WRITE] 1949/pi_vol_5_1_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=54\n",
      "[WRITE] 1949/pi_vol_5_4_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=36\n",
      "[WRITE] 1949/pi_vol_5_6_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=50\n",
      "[WRITE] 1949/pi_vol_4_5_1949-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=47\n",
      "[WRITE] 1948/pi_16_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=43\n",
      "[WRITE] 1948/pi_vol_3_6_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=43\n",
      "[WRITE] 1948/pi_19_first_edition_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=40\n",
      "[WRITE] 1948/pi_19_second_edition_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=48\n",
      "[WRITE] 1948/pi_vol_3_3_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=37\n",
      "[WRITE] 1948/pi_21_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=43\n",
      "[WRITE] 1948/pi_vol_3_1_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=33\n",
      "[WRITE] 1948/pi_vol_3_2_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=48\n",
      "[WRITE] 1948/pi_18_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=52\n",
      "[WRITE] 1948/pi_20_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=41\n",
      "[WRITE] 1948/pi_vol_3_5_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=50\n",
      "[WRITE] 1948/pi_17_1948-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=45\n",
      "[WRITE] 1946/pi_4_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=35\n",
      "[WRITE] 1946/pi_vol_2_6_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=37\n",
      "[WRITE] 1946/pi_vol_2_3_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=36\n",
      "[WRITE] 1946/pi_3_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=27\n",
      "[WRITE] 1946/pi_vol_2_1_1946-cleaned.json  pages=[1, 2, 3, 4, 5, 6]  mode=json  articles=43\n",
      "[WRITE] 1946/pi_1_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=25\n",
      "[WRITE] 1946/pi_vol_2_4_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=36\n",
      "[WRITE] 1946/pi_vol_2_2_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=40\n",
      "[WRITE] 1946/pi_2_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=45\n",
      "[WRITE] 1946/pi_vol_2_5_1946-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=35\n",
      "[WRITE] 1952/pi_vol_9_9_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=41\n",
      "[WRITE] 1952/pi_vol_10_1_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=37\n",
      "[WRITE] 1952/pi_vol_10_4_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=20\n",
      "[WRITE] 1952/pi_vol_9_6_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=36\n",
      "[WRITE] 1952/pi_vol_10_3_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=53\n",
      "[WRITE] 1952/pi_vol_9_7_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=46\n",
      "[WRITE] 1952/pi_vol_9_5_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=27\n",
      "[WRITE] 1952/pi_vol_10_2_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=49\n",
      "[WRITE] 1952/pi_vol_9_8_1952-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=55\n",
      "[WRITE] 1954/pi_vol_11_6_1954-cleaned.json  pages=[1, 2, 3, 4, 5, 6]  mode=json  articles=49\n",
      "[WRITE] 1954/pi_vol_11_8_1954-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=30\n",
      "[WRITE] 1954/pi_vol_11_5_1954-cleaned.json  pages=[1, 2, 3, 4, 5, 6]  mode=json  articles=56\n",
      "[WRITE] 1954/pi_vol_11_7_1954-cleaned.json  pages=[1, 2, 3, 4, 5, 6]  mode=json  articles=61\n",
      "[WRITE] 1953/pi_vol_11_1_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=42\n",
      "[WRITE] 1953/pi_vol_11_3_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=44\n",
      "[WRITE] 1953/pi_vol_10_5_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=45\n",
      "[WRITE] 1953/pi_vol_10_8_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=46\n",
      "[WRITE] 1953/pi_vol_11_4_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=38\n",
      "[WRITE] 1953/pi_vol_10_7_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=25\n",
      "[WRITE] 1953/pi_vol_10_9_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=52\n",
      "[WRITE] 1953/pi_vol_10_6_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=35\n",
      "[WRITE] 1953/pi_vol_11_2_1953-cleaned.json  pages=[1, 2, 3, 4]  mode=json  articles=45\n",
      "Done. Wrote 87 file(s).\n"
     ]
    }
   ],
   "source": [
    "# Combine multi-page OCR JSONs into one file per issue.\n",
    "# Jupyter-friendly: hard-coded paths, no CLI args.\n",
    "\n",
    "import os, re, json\n",
    "from json import JSONDecodeError\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Union, Any\n",
    "\n",
    "# ---- EDIT THESE TWO LINES IF YOUR PATHS CHANGE ----\n",
    "INPUT_ROOT  = \"/Users/stepanyan/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Knowledge Exchange and Innovation - UCL - Wikimedia - Shared working space/NER/Newswire - Jupyter/Pi-OCR-Articles-Cleaned\"\n",
    "OUTPUT_ROOT = \"/Users/stepanyan/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Knowledge Exchange and Innovation - UCL - Wikimedia - Shared working space/NER/Newswire - Jupyter/Pi-OCR-Articles-Cleaned-Combined\"\n",
    "# ---------------------------------------------------\n",
    "\n",
    "OVERWRITE = False      # set True to overwrite existing combined files\n",
    "DRY_RUN   = False      # set True to preview without writing\n",
    "\n",
    "PAGE_FILE_RE = re.compile(r\"^(?P<base>.+?)_(?P<page>\\d{3})-cleaned\\.json$\")\n",
    "\n",
    "def assert_paths():\n",
    "    if not os.path.isdir(INPUT_ROOT):\n",
    "        raise FileNotFoundError(f\"INPUT_ROOT not found: {INPUT_ROOT}\")\n",
    "    # Prevent accidental recursion if OUTPUT_ROOT is inside INPUT_ROOT\n",
    "    in_abs  = os.path.abspath(INPUT_ROOT)\n",
    "    out_abs = os.path.abspath(OUTPUT_ROOT)\n",
    "    if out_abs.startswith(in_abs + os.sep):\n",
    "        raise ValueError(\"OUTPUT_ROOT must not be inside INPUT_ROOT. Choose a sibling or separate folder.\")\n",
    "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "def mirror_dir_structure(in_root: str, out_root: str, dry_run: bool = False):\n",
    "    for dirpath, dirnames, _ in os.walk(in_root):\n",
    "        rel = os.path.relpath(dirpath, start=in_root)\n",
    "        target_dir = os.path.join(out_root, rel) if rel != os.curdir else out_root\n",
    "        if not dry_run:\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "def find_issue_groups(root: str):\n",
    "    \"\"\"\n",
    "    Return mapping: (dirpath, base) -> list of (page_num, abs_path, dirpath, filename, base)\n",
    "    Only matches files like <base>_NNN-cleaned.json\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        for fn in files:\n",
    "            if not fn.endswith(\"-cleaned.json\"):\n",
    "                continue\n",
    "            m = PAGE_FILE_RE.match(fn)\n",
    "            if not m:\n",
    "                # Skip already-combined files like <base>-cleaned.json (no _NNN)\n",
    "                continue\n",
    "            base = m.group(\"base\")\n",
    "            page = int(m.group(\"page\"))\n",
    "            abs_path = os.path.join(dirpath, fn)\n",
    "            groups[(dirpath, base)].append((page, abs_path, dirpath, fn, base))\n",
    "    # Sort by page number\n",
    "    for k in groups:\n",
    "        groups[k].sort(key=lambda x: x[0])\n",
    "    return groups\n",
    "\n",
    "def load_json_or_text(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        try:\n",
    "            return json.load(fh), \"json\"\n",
    "        except JSONDecodeError:\n",
    "            fh.seek(0)\n",
    "            return fh.read(), \"text\"\n",
    "\n",
    "def _extract_articles(data: Any) -> list:\n",
    "    \"\"\"\n",
    "    Normalize a single page payload into a list of articles:\n",
    "      - {\"articles\": [...]} -> [...]\n",
    "      - [...]                -> [...]\n",
    "      - {...}                -> [ {...} ]\n",
    "      - other                -> []\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if \"articles\" in data:\n",
    "            arts = data[\"articles\"]\n",
    "            return arts if isinstance(arts, list) else [arts]\n",
    "        return [data]\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    return []\n",
    "\n",
    "def combine_pages(paged_files: List[Tuple[int, str]]) -> Tuple[Union[dict, str], str]:\n",
    "    \"\"\"\n",
    "    Returns (combined, mode)\n",
    "      - mode == 'json'  -> {\"articles\": [...]} where each article has 'id' and 'page'\n",
    "      - mode == 'text'  -> newline-delimited fallback when any page is non-JSON\n",
    "    \"\"\"\n",
    "    parsed = []\n",
    "    all_json = True\n",
    "    for page, path in paged_files:\n",
    "        data, kind = load_json_or_text(path)\n",
    "        parsed.append((page, data, kind))\n",
    "        if kind != \"json\":\n",
    "            all_json = False\n",
    "\n",
    "    if not all_json:\n",
    "        # Fallback: newline-delimited serialization (no normalization possible)\n",
    "        lines = []\n",
    "        for page, data, kind in parsed:\n",
    "            if kind == \"json\":\n",
    "                lines.append(json.dumps(data, ensure_ascii=False))\n",
    "            else:\n",
    "                lines.append(str(data))\n",
    "        return \"\\n\".join(lines), \"text\"\n",
    "\n",
    "    # All JSON: normalize, annotate with page, then assign sequential ids\n",
    "    combined_articles = []\n",
    "    for page, data, _ in parsed:\n",
    "        for art in _extract_articles(data):\n",
    "            # Ensure each article is a dict; if not, preserve raw under \"_raw\"\n",
    "            if not isinstance(art, dict):\n",
    "                art = {\"_raw\": art}\n",
    "            # Work on a shallow copy to avoid mutating inputs\n",
    "            art = dict(art)\n",
    "            # Always set/overwrite page to the source page number\n",
    "            art[\"page\"] = page\n",
    "            combined_articles.append(art)\n",
    "\n",
    "    # Assign unique sequential IDs starting from 1 across the issue\n",
    "    for i, art in enumerate(combined_articles, start=1):\n",
    "        art[\"id\"] = i\n",
    "\n",
    "    return {\"articles\": combined_articles}, \"json\"\n",
    "\n",
    "def ensure_out_dir(in_root: str, dirpath: str, out_root: str) -> str:\n",
    "    rel = os.path.relpath(dirpath, start=in_root)\n",
    "    target_dir = os.path.join(out_root, rel) if rel != os.curdir else out_root\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    return target_dir\n",
    "\n",
    "# ---- run ----\n",
    "print(f\"[INPUT ROOT]  {INPUT_ROOT}\")\n",
    "print(f\"[OUTPUT ROOT] {OUTPUT_ROOT}\")\n",
    "\n",
    "assert_paths()\n",
    "mirror_dir_structure(INPUT_ROOT, OUTPUT_ROOT, dry_run=DRY_RUN)\n",
    "\n",
    "groups = find_issue_groups(INPUT_ROOT)\n",
    "if not groups:\n",
    "    print(\"No page groups found under INPUT_ROOT.\")\n",
    "else:\n",
    "    total_written = 0\n",
    "    for (dirpath, base), entries in groups.items():\n",
    "        pages = [e[0] for e in entries]\n",
    "        file_paths = [e[1] for e in entries]\n",
    "        out_dir = ensure_out_dir(INPUT_ROOT, dirpath, OUTPUT_ROOT)\n",
    "        out_filename = f\"{base}-cleaned.json\"  # remove the _NNN segment\n",
    "        out_path = os.path.join(out_dir, out_filename)\n",
    "\n",
    "        if os.path.exists(out_path) and not OVERWRITE:\n",
    "            print(f\"[SKIP] exists: {os.path.relpath(out_path, start=OUTPUT_ROOT)}\")\n",
    "            continue\n",
    "\n",
    "        # Pass (page, path) pairs so we can annotate 'page' during normalization\n",
    "        paged_files = list(zip(pages, file_paths))\n",
    "        combined, mode = combine_pages(paged_files)\n",
    "\n",
    "        rel_out = os.path.relpath(out_path, start=OUTPUT_ROOT)\n",
    "        print(f\"[WRITE] {rel_out}  pages={pages}  mode={mode}  articles={len(combined['articles']) if mode=='json' else 'N/A'}\")\n",
    "\n",
    "        if not DRY_RUN:\n",
    "            if mode == \"json\":\n",
    "                with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    json.dump(combined, fh, ensure_ascii=False, indent=2)\n",
    "                    fh.write(\"\\n\")\n",
    "            else:\n",
    "                with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(combined)\n",
    "        total_written += 1\n",
    "\n",
    "    print(f\"Done. {'Would write' if DRY_RUN else 'Wrote'} {total_written} file(s).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
