{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd7cd60",
   "metadata": {},
   "source": [
    "### Semantic Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63d26a",
   "metadata": {},
   "source": [
    "#### 0. Environment Setup\n",
    "- Resolve `BASE_DIR` to the folder containing this notebook.\n",
    "- Define input/output roots:\n",
    "  - `INPUT_ROOT = output-issues-combined-cleaned` (from the previous notebook)\n",
    "  - `OUTPUT_ROOT = output-semantic` and `CACHE_DIR = output-semantic/cache`\n",
    "- Create missing folders and print helpful diagnostics if inputs are absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1784d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] requests-cache enabled\n",
      "[ok] spaCy model: en_core_web_trf\n",
      "[ok] geopy + Nominatim ready\n",
      "[ok] rapidfuzz ready\n"
     ]
    }
   ],
   "source": [
    "# --- Environment & imports (consolidated) -------------------------------------\n",
    "# Optional installs (uncomment if needed)\n",
    "# %pip install -q spacy geopy requests-cache rapidfuzz tqdm pandas\n",
    "# %pip install -q \"spacy-transformers>=1.2.5\"     # if you want en_core_web_trf\n",
    "# python -m spacy download en_core_web_sm         # run once if 'sm' is missing\n",
    "# python -m spacy download en_core_web_trf        # run once if 'trf' is missing\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, json, time, math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# If previous cell didn't define these, set safe defaults\n",
    "try:\n",
    "    BASE_DIR\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "try:\n",
    "    OUTPUT_ROOT\n",
    "except NameError:\n",
    "    OUTPUT_ROOT = BASE_DIR / \"output-semantic\"\n",
    "try:\n",
    "    CACHE_DIR\n",
    "except NameError:\n",
    "    CACHE_DIR = OUTPUT_ROOT / \"cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# HTTP caching\n",
    "try:\n",
    "    import requests_cache\n",
    "    requests_cache.install_cache(str(CACHE_DIR / \"http_cache\"), expire_after=60*60*24*14)\n",
    "    print(\"[ok] requests-cache enabled\")\n",
    "except Exception as e:\n",
    "    print(f\"[warn] requests-cache not active: {e}\")\n",
    "\n",
    "# spaCy: prefer transformer, fall back to small model\n",
    "import spacy\n",
    "def _load_spacy():\n",
    "    for name in (\"en_core_web_trf\", \"en_core_web_sm\"):\n",
    "        try:\n",
    "            return spacy.load(name), name\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(\n",
    "        \"No spaCy English model installed.\\n\"\n",
    "        \"Install one of:\\n\"\n",
    "        \"  python -m spacy download en_core_web_sm\\n\"\n",
    "        \"  python -m spacy download en_core_web_trf\"\n",
    "    )\n",
    "nlp, NER_MODEL = _load_spacy()\n",
    "print(f\"[ok] spaCy model: {NER_MODEL}\")\n",
    "\n",
    "# Geopy (Nominatim) with rate limiting\n",
    "try:\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    USER_EMAIL = os.getenv(\"NOMINATIM_EMAIL\", \"example@example.com\")\n",
    "    geolocator = Nominatim(user_agent=f\"pi-semantic/0.1 ({USER_EMAIL})\", timeout=15)\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.0, swallow_exceptions=True)\n",
    "    print(\"[ok] geopy + Nominatim ready\")\n",
    "except Exception as e:\n",
    "    geolocator = None\n",
    "    geocode = None\n",
    "    print(f\"[warn] geopy not available: {e}\")\n",
    "\n",
    "# Fuzzy matching\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    print(\"[ok] rapidfuzz ready\")\n",
    "except Exception as e:\n",
    "    fuzz = None\n",
    "    print(f\"[warn] rapidfuzz not available: {e}\")\n",
    "\n",
    "tqdm.pandas(disable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e35c49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 0. Setup & configuration --------------------------------------------------\n",
    "# from pathlib import Path\n",
    "# import os, re, json, gzip, time, math, glob, random, textwrap, itertools\n",
    "# from collections import defaultdict, Counter\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas(disable=False)\n",
    "\n",
    "# # Resolve this notebook's directory (works in Jupyter and if exported to .py)\n",
    "# if \"__file__\" in globals():                      # running as a script\n",
    "#     BASE_DIR = Path(__file__).resolve().parent\n",
    "# else:                                            # running in Jupyter\n",
    "#     BASE_DIR = Path.cwd()\n",
    "\n",
    "# # Inputs/outputs\n",
    "# # - INPUT_ROOT: combined issues from the previous notebook\n",
    "# # - OUTPUT_ROOT: fresh working area for all semantic outputs (and cache)\n",
    "# INPUT_ROOT  = BASE_DIR / \"output-issues-combined-cleaned\"\n",
    "# OUTPUT_ROOT = BASE_DIR / \"output-semantic\"\n",
    "# CACHE_DIR   = OUTPUT_ROOT / \"cache\"\n",
    "\n",
    "# # Create outputs if missing\n",
    "# for p in (OUTPUT_ROOT, CACHE_DIR):\n",
    "#     p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Sanity check: help the user if inputs are missing\n",
    "# if not INPUT_ROOT.exists():\n",
    "#     print(f\"[warn] INPUT_ROOT does not exist: {INPUT_ROOT}\")\n",
    "#     print(\"       Did you run the previous notebook to produce combined issues?\")\n",
    "# else:\n",
    "#     print(f\"[ok] Found INPUT_ROOT: {INPUT_ROOT}\")\n",
    "\n",
    "# print(\"BASE_DIR   :\", BASE_DIR)\n",
    "# print(\"OUTPUT_ROOT:\", OUTPUT_ROOT)\n",
    "# print(\"CACHE_DIR  :\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5e1ba",
   "metadata": {},
   "source": [
    "### 1 Environment (installs)\n",
    "This step is done further down the notebook, as the versions in the code below were incompatible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "106d040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light stack; British English (1940s–50s) will use general English models.\n",
    "# %pip -q install spacy spacy-transformers transformers requests requests-cache geopy rapidfuzz\n",
    "\n",
    "# # spaCy model (transformer is accurate; small corpus so speed is OK)\n",
    "# !python -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43c57a",
   "metadata": {},
   "source": [
    "#### 1. Load combined JSONs → normalized rows\n",
    "- Recursively scan `output-issues-combined-cleaned` for `*-cleaned.json`.\n",
    "- Accept various shapes: `{\"articles\":[...]}`, list, or object.\n",
    "- Normalize into a DataFrame with: `issue_id`, `id`, `page`, `year`, `title`, `text`, `source_file`.\n",
    "- Keep the original record under `orig` for later optional fields (author/date/etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f27dfbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Loaded 3 articles from 1 issue file(s).\n",
      "[sanity] Wrapper-like rows in text: 0 (should be 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_id</th>\n",
       "      <th>source_file</th>\n",
       "      <th>source_path</th>\n",
       "      <th>unit_index</th>\n",
       "      <th>id</th>\n",
       "      <th>page</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pi-Newspaper-1979</td>\n",
       "      <td>Pi-Newspaper-1979-cleaned.json</td>\n",
       "      <td>/Users/stepanyan/Documents/UCL/GitHub-Projects...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>HOW SAFE ARE OUR HALLS?</td>\n",
       "      <td>Recent months have seen a spate of thefts at I...</td>\n",
       "      <td>{'id': 1, 'title': 'HOW SAFE ARE OUR HALLS?', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pi-Newspaper-1979</td>\n",
       "      <td>Pi-Newspaper-1979-cleaned.json</td>\n",
       "      <td>/Users/stepanyan/Documents/UCL/GitHub-Projects...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>UCL IN SPACE?</td>\n",
       "      <td>Skylab II, the second orbital space laboratory...</td>\n",
       "      <td>{'id': 2, 'title': 'UCL IN SPACE?', 'author': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pi-Newspaper-1979</td>\n",
       "      <td>Pi-Newspaper-1979-cleaned.json</td>\n",
       "      <td>/Users/stepanyan/Documents/UCL/GitHub-Projects...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>KING'S v NUS</td>\n",
       "      <td>A nationwide campaign for the reform of NUS ma...</td>\n",
       "      <td>{'id': 3, 'title': 'KING'S v NUS', 'author': N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            issue_id                     source_file  \\\n",
       "0  Pi-Newspaper-1979  Pi-Newspaper-1979-cleaned.json   \n",
       "1  Pi-Newspaper-1979  Pi-Newspaper-1979-cleaned.json   \n",
       "2  Pi-Newspaper-1979  Pi-Newspaper-1979-cleaned.json   \n",
       "\n",
       "                                         source_path  unit_index  id  page  \\\n",
       "0  /Users/stepanyan/Documents/UCL/GitHub-Projects...           1   1     1   \n",
       "1  /Users/stepanyan/Documents/UCL/GitHub-Projects...           2   2     1   \n",
       "2  /Users/stepanyan/Documents/UCL/GitHub-Projects...           3   3     1   \n",
       "\n",
       "   year                    title  \\\n",
       "0  1979  HOW SAFE ARE OUR HALLS?   \n",
       "1  1979            UCL IN SPACE?   \n",
       "2  1979             KING'S v NUS   \n",
       "\n",
       "                                                text  \\\n",
       "0  Recent months have seen a spate of thefts at I...   \n",
       "1  Skylab II, the second orbital space laboratory...   \n",
       "2  A nationwide campaign for the reform of NUS ma...   \n",
       "\n",
       "                                                orig  \n",
       "0  {'id': 1, 'title': 'HOW SAFE ARE OUR HALLS?', ...  \n",
       "1  {'id': 2, 'title': 'UCL IN SPACE?', 'author': ...  \n",
       "2  {'id': 3, 'title': 'KING'S v NUS', 'author': N...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ISSUE_FILE_RX = re.compile(r\"^(?P<issue>.+?)-cleaned\\.json$\", re.IGNORECASE)\n",
    "YEAR_RX       = re.compile(r\"\\b(19|20)\\d{2}\\b\")\n",
    "\n",
    "TITLE_KEYS = (\"title\",\"headline\",\"heading\")\n",
    "TEXT_KEYS  = (\"content\",\"body\",\"text\",\"article\",\"main_text\",\"raw_text\")\n",
    "\n",
    "def iter_records_from_obj(obj):\n",
    "    \"\"\"Yield article dicts from various container shapes.\"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        for it in obj:\n",
    "            yield it\n",
    "    elif isinstance(obj, dict):\n",
    "        if \"articles\" in obj and isinstance(obj[\"articles\"], list):\n",
    "            for it in obj[\"articles\"]:\n",
    "                yield it\n",
    "        else:\n",
    "            yield obj\n",
    "    else:\n",
    "        yield {\"raw_text\": str(obj)}\n",
    "\n",
    "def read_any_json(path: Path):\n",
    "    \"\"\"Read JSON or JSONL reliably; yield dict-like article records.\"\"\"\n",
    "    txt = path.read_text(encoding=\"utf-8\").strip()\n",
    "    # Try full JSON first\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        for rec in iter_records_from_obj(obj):\n",
    "            yield rec\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # Fallback: JSONL\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            obj = {\"raw_text\": line}\n",
    "        yield from iter_records_from_obj(obj)\n",
    "\n",
    "def pick_title_text(d: dict):\n",
    "    \"\"\"Extract (title, body) from a single article record.\"\"\"\n",
    "    title = next((d[k].strip() for k in TITLE_KEYS\n",
    "                  if isinstance(d.get(k), str) and d[k].strip()), None)\n",
    "    body  = next((d[k].strip() for k in TEXT_KEYS\n",
    "                  if isinstance(d.get(k), str) and d[k].strip()), None)\n",
    "    if body is None and any(k in d for k in (\"paragraphs\",\"content_blocks\")):\n",
    "        body = json.dumps({k: d.get(k) for k in (\"paragraphs\",\"content_blocks\")}, ensure_ascii=False)\n",
    "    return title, (body or \"\")\n",
    "\n",
    "def issue_id_from_path(p: Path) -> str:\n",
    "    \"\"\"Use filename stem minus '-cleaned' as the issue id (works for both hyphen/underscore page schemes).\"\"\"\n",
    "    m = ISSUE_FILE_RX.match(p.name)\n",
    "    return m.group(\"issue\") if m else p.stem.replace(\"-cleaned\", \"\")\n",
    "\n",
    "def year_from_issue_or_meta(issue_id: str, article: dict):\n",
    "    \"\"\"Try to infer a year from the filename or article metadata.\"\"\"\n",
    "    # 1) from issue_id (e.g., 'Pi-Newspaper-1948-Vol2')\n",
    "    m = YEAR_RX.search(issue_id)\n",
    "    if m:\n",
    "        return int(m.group(0))\n",
    "    # 2) from date-like fields (very light heuristic)\n",
    "    for k in (\"date\",\"pub_date\",\"published\",\"issue_date\"):\n",
    "        v = article.get(k)\n",
    "        if isinstance(v, str):\n",
    "            m = YEAR_RX.search(v)\n",
    "            if m:\n",
    "                return int(m.group(0))\n",
    "    return None\n",
    "\n",
    "def collect_articles(root: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    files = sorted(root.rglob(\"*-cleaned.json\"))\n",
    "    if not files:\n",
    "        print(f\"[warn] No '*-cleaned.json' files found under: {root}\")\n",
    "    for f in files:\n",
    "        issue_id = issue_id_from_path(f)\n",
    "        for i, art in enumerate(read_any_json(f), start=1):\n",
    "            title, body = pick_title_text(art)\n",
    "            art_id = art.get(\"id\", i)\n",
    "            page   = art.get(\"page\")\n",
    "            year   = year_from_issue_or_meta(issue_id, art)\n",
    "            rows.append({\n",
    "                \"issue_id\": issue_id,\n",
    "                \"source_file\": f.name,\n",
    "                \"source_path\": str(f),\n",
    "                \"unit_index\": i,\n",
    "                \"id\": art_id,\n",
    "                \"page\": page,\n",
    "                \"year\": year,\n",
    "                \"title\": title,\n",
    "                \"text\": body,\n",
    "                \"orig\": art,  # keep original for optional fields (author/date/etc.)\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"[ok] Loaded {len(df)} articles from {len(files)} issue file(s).\")\n",
    "    return df\n",
    "\n",
    "df = collect_articles(INPUT_ROOT)\n",
    "\n",
    "# sanity: make sure text is not an articles-wrapper\n",
    "wrappers = df[\"text\"].astype(str).str.startswith('{\"articles\"').sum()\n",
    "print(f\"[sanity] Wrapper-like rows in text: {wrappers} (should be 0)\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c7bfc",
   "metadata": {},
   "source": [
    "### 2. Light OCR cleanup (conservative)\n",
    "- Unicode normalization, strip control chars (preserve whitespace).\n",
    "- Curly → straight quotes; ligature expansion; odd spaces → normal spaces.\n",
    "- Normalize dashes; optional de-hyphenation across line breaks (text only).\n",
    "- Create `title_norm` (no de-hyphenation) and `text_norm` (with de-hyphenation).\n",
    "- Print quick change counts for sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ac5d124b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979</td>\n",
       "      <td>HOW SAFE ARE OUR HALLS?</td>\n",
       "      <td>2289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979</td>\n",
       "      <td>UCL IN SPACE?</td>\n",
       "      <td>1683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>KING'S v NUS</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year              title_clean  char_len\n",
       "0  1979  HOW SAFE ARE OUR HALLS?      2289\n",
       "1  1979            UCL IN SPACE?      1683\n",
       "2  1979             KING'S v NUS       636"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_ocr(t: str) -> str:\n",
    "    # soft hyphen at EOL, hard hyphenation joins, normalize quotes & spaces\n",
    "    t = t.replace(\"\\u00ad\\n\", \"\")\n",
    "    t = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", t)\n",
    "    t = re.sub(r\"[“”]\", '\"', t).replace(\"’\", \"'\")\n",
    "    t = re.sub(r\"[ \\t]+\\n\", \"\\n\", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].map(clean_ocr)\n",
    "df[\"title_clean\"] = df[\"title\"].map(lambda s: clean_ocr(s) if isinstance(s,str) else None)\n",
    "df[\"char_len\"] = df[\"text_clean\"].str.len()\n",
    "df[[\"year\",\"title_clean\",\"char_len\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0496be",
   "metadata": {},
   "source": [
    "### 4. Geocoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "632b20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U typing_extensions\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "39b8e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \"pydantic>=2.7\" \"spacy>=3.7.4\"\n",
    "# %pip install -U \"typing_extensions>=4.12\"\n",
    "# %pip install -U \"spacy>=3.7.2\" \"pydantic>=2,<3\" spacy-transformers transformers torch\n",
    "# # (Re)install an English model:\n",
    "# !python -m spacy download en_core_web_trf\n",
    "\n",
    "\n",
    "# import sys, importlib.metadata as im\n",
    "\n",
    "# print(\"Python:\", sys.version)\n",
    "# print(\"Kernel executable:\", sys.executable)\n",
    "# for pkg in (\"spacy\", \"pydantic\", \"pydantic-core\", \"thinc\"):\n",
    "#     try:\n",
    "#         print(f\"{pkg}:\", im.version(pkg))\n",
    "#     except im.PackageNotFoundError:\n",
    "#         print(f\"{pkg}: not installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ed613",
   "metadata": {},
   "source": [
    "### 4A — Extract location mentions (NER + heuristics)\n",
    "- Use spaCy NER on `title_norm` + `text_norm` to collect `GPE`/`LOC`/`FAC`.\n",
    "- Optional regex add-ons (e.g., UK postcodes, street names).\n",
    "- Deduplicate mentions per article (case/punct-insensitive), preserving order.\n",
    "- Store:\n",
    "  - `loc_mentions`: detailed dicts `{text, label, start, end, source}`\n",
    "  - `loc_texts`: unique mention strings for quick inspection\n",
    "  \n",
    "RUN TIME - ~14 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ed52993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOW SAFE ARE OUR HALLS?</td>\n",
       "      <td>[{'text': 'Max Rayne House', 'start': 65, 'end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCL IN SPACE?</td>\n",
       "      <td>[{'text': 'Skylab II', 'start': 0, 'end': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KING'S v NUS</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title_clean                                  location_mentions\n",
       "0  HOW SAFE ARE OUR HALLS?  [{'text': 'Max Rayne House', 'start': 65, 'end...\n",
       "1            UCL IN SPACE?  [{'text': 'Skylab II', 'start': 0, 'end': 9, '...\n",
       "2             KING'S v NUS                                                 []"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Step 4A: Extract all location mentions across the full text ----\n",
    "import re, json\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure an English spaCy pipeline is loaded (we'll try transformer; fall back to small)\n",
    "try:\n",
    "    nlp\n",
    "except NameError:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_trf\")\n",
    "    except Exception:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Simple UK/EN street patterns (extend as needed)\n",
    "STREET_SUFFIXES = r\"(Street|St\\.|Road|Rd\\.|Avenue|Ave\\.|Lane|Ln\\.|Close|Way|Square|Sq\\.|Terrace|Terr\\.|Place|Pl\\.|Drive|Dr\\.|Crescent|Cresc\\.|Court|Ct\\.|Row|Quay|Embankment|Parade)\"\n",
    "STREET_RE = re.compile(rf\"\\b([A-Z][a-zA-Z'-]+(?:\\s+[A-Z][a-zA-Z'-]+)*)\\s+{STREET_SUFFIXES}\\b\")\n",
    "\n",
    "def extract_location_mentions(text: str):\n",
    "    \"\"\"\n",
    "    Returns a list of candidate location mentions with offsets and a coarse 'kind'.\n",
    "    Kinds: 'street', 'place' (GPE/LOC), 'facility' (FAC), 'country' (if resolved later).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return out\n",
    "\n",
    "    # 1) NER for GPE/LOC/FAC across entire text\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (\"GPE\",\"LOC\",\"FAC\"):\n",
    "            out.append({\n",
    "                \"text\": ent.text.strip(\".,;:()[]\\\"' \"),\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char,\n",
    "                \"kind\": \"place\" if ent.label_ in (\"GPE\",\"LOC\") else \"facility\",\n",
    "                \"source\": \"ner\"\n",
    "            })\n",
    "\n",
    "    # 2) Street patterns (regex), capture overlaps that spaCy might miss\n",
    "    for m in STREET_RE.finditer(text):\n",
    "        span = m.group(0).strip(\".,;:()[]\\\"' \")\n",
    "        out.append({\n",
    "            \"text\": span,\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"kind\": \"street\",\n",
    "            \"source\": \"regex\"\n",
    "        })\n",
    "\n",
    "    # 3) Normalize & deduplicate near-duplicates (case-insensitive)\n",
    "    # Keep first occurrence offsets\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for m in out:\n",
    "        key = (m[\"text\"].lower(), m[\"kind\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(m)\n",
    "\n",
    "    return deduped\n",
    "\n",
    "# Apply to all rows\n",
    "df[\"location_mentions\"] = df[\"text_clean\"].map(extract_location_mentions)\n",
    "\n",
    "# Quick peek\n",
    "df[[\"title_clean\",\"location_mentions\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ef471",
   "metadata": {},
   "source": [
    "### 4B. Geocode → structured locations (cache & rate limiting)\n",
    "- Geocode unique mentions with Nominatim (via `geopy`), cached on disk + HTTP cache.\n",
    "- Map OSM address to fields: `street`, `city`, `state`, `country`, `postcode`, `coordinates`, `location_notes`.\n",
    "- Preserve provenance (`geo` block) for QA.\n",
    "- Assemble per-article `locations_structured` (dedup by key fields).\n",
    "\n",
    "RUN TIME ~35 mins. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "78c92891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>resolved_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOW SAFE ARE OUR HALLS?</td>\n",
       "      <td>[{'label': 'Max Rayne House', 'type': 'facilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCL IN SPACE?</td>\n",
       "      <td>[{'label': 'Skylab II', 'type': 'facility', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KING'S v NUS</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title_clean                                 resolved_locations\n",
       "0  HOW SAFE ARE OUR HALLS?  [{'label': 'Max Rayne House', 'type': 'facilit...\n",
       "1            UCL IN SPACE?  [{'label': 'Skylab II', 'type': 'facility', 's...\n",
       "2             KING'S v NUS                                                 []"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Step 4B: Geocode to structured fields with cache & rate limiter ----\n",
    "import requests, requests_cache, json, time\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from pathlib import Path\n",
    "\n",
    "# Persistent HTTP cache (2 weeks)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "requests_cache.install_cache(str(CACHE_DIR / \"http_cache\"), expire_after=60*60*24*14)\n",
    "\n",
    "# Local JSONL geocode cache\n",
    "GEOCODE_CACHE = CACHE_DIR / \"all_locations_geocode_cache.jsonl\"\n",
    "_geo_cache = {}\n",
    "if GEOCODE_CACHE.exists():\n",
    "    with open(GEOCODE_CACHE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            _geo_cache[(rec[\"query\"].lower(), rec.get(\"country_bias\"))] = rec\n",
    "\n",
    "def _cache_geocode_write(key, rec):\n",
    "    _geo_cache[key] = rec\n",
    "    with open(GEOCODE_CACHE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Single shared Nominatim client with rate limiting\n",
    "_geolocator = Nominatim(user_agent=\"pi-newswire-alllocs\", timeout=15)\n",
    "_geocode = RateLimiter(_geolocator.geocode, min_delay_seconds=1.0, swallow_exceptions=True)\n",
    "\n",
    "def normalize_address(addr: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Map OSM address fields to our target schema:\n",
    "      street, council, city, state, region, country, postcode\n",
    "    (We keep both 'state' and 'region' for compatibility.)\n",
    "    \"\"\"\n",
    "    if not addr:\n",
    "        return {}\n",
    "\n",
    "    def pick(*keys):\n",
    "        for k in keys:\n",
    "            v = addr.get(k)\n",
    "            if v:\n",
    "                return v\n",
    "        return None\n",
    "\n",
    "    street   = pick(\"road\", \"pedestrian\", \"residential\", \"footway\", \"path\", \"neighbourhood\", \"street\", \"cycleway\")\n",
    "    council  = pick(\"borough\", \"municipality\", \"city_district\", \"district\", \"county\", \"local_authority\", \"state_district\")\n",
    "    city     = pick(\"city\", \"town\", \"village\", \"hamlet\", \"suburb\")\n",
    "    state    = pick(\"state\", \"region\", \"province\")\n",
    "    country  = pick(\"country\")\n",
    "    postcode = pick(\"postcode\")\n",
    "\n",
    "    return {\n",
    "        \"street\": street,\n",
    "        \"council\": council,\n",
    "        \"city\": city,\n",
    "        \"state\": state,          # new explicit field\n",
    "        \"region\": state,         # keep old name for compatibility\n",
    "        \"country\": country,\n",
    "        \"postcode\": postcode,    # new\n",
    "    }\n",
    "\n",
    "def geocode_text(query: str, country_bias: str|None=None) -> dict|None:\n",
    "    \"\"\"\n",
    "    Geocode a text mention via Nominatim, returning normalized address + lat/lon.\n",
    "    country_bias: e.g., 'gb' to bias to the UK for ambiguous street names.\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return None\n",
    "    key = (query.lower(), country_bias)\n",
    "    if key in _geo_cache:\n",
    "        return _geo_cache[key]\n",
    "\n",
    "    try:\n",
    "        loc = _geocode(query, addressdetails=True, country_codes=country_bias)\n",
    "    except Exception:\n",
    "        loc = None\n",
    "\n",
    "    if not loc:\n",
    "        rec = {\"query\": query, \"country_bias\": country_bias, \"ok\": False}\n",
    "        _cache_geocode_write(key, rec)\n",
    "        return rec\n",
    "\n",
    "    addr = normalize_address(getattr(loc, \"raw\", {}).get(\"address\", {}))\n",
    "    rec = {\n",
    "        \"query\": query,\n",
    "        \"country_bias\": country_bias,\n",
    "        \"ok\": True,\n",
    "        \"lat\": loc.latitude,\n",
    "        \"lon\": loc.longitude,\n",
    "        \"address\": addr,\n",
    "        \"display_name\": loc.address,\n",
    "        \"source\": \"nominatim\"\n",
    "    }\n",
    "    _cache_geocode_write(key, rec)\n",
    "    return rec\n",
    "\n",
    "def geocode_mentions_for_row(mentions, prefer_uk_streets=True):\n",
    "    \"\"\"\n",
    "    For each mention, geocode and return a list of resolved locations.\n",
    "\n",
    "    Adds:\n",
    "      - label: original mention text (e.g., \"Max Rayne House\")\n",
    "      - type:  NER label (FAC/GPE/LOC) if available, else 'kind'\n",
    "      - street/state/postcode\n",
    "      - coordinates: [lat, lon]\n",
    "      - location_notes: same as display_name\n",
    "\n",
    "    Keeps existing fields for compatibility (mention, kind, lat, lon, display_name, geo_source, country_bias, council/region/city/country).\n",
    "    \"\"\"\n",
    "    if not mentions:\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    seen_keys = set()\n",
    "\n",
    "    for m in mentions:\n",
    "        if not isinstance(m, dict):\n",
    "            continue\n",
    "\n",
    "        q = (m.get(\"text\") or \"\").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        kind = m.get(\"kind\", \"place\")\n",
    "        ner_type = m.get(\"label\") or kind   # prefer FAC/GPE/LOC if present\n",
    "        bias = \"gb\" if (kind == \"street\" and prefer_uk_streets) else None\n",
    "\n",
    "        rec = geocode_text(q, country_bias=bias)\n",
    "        if not rec or not rec.get(\"ok\"):\n",
    "            continue\n",
    "\n",
    "        addr = rec.get(\"address\", {}) or {}\n",
    "        lat, lon = rec.get(\"lat\"), rec.get(\"lon\")\n",
    "\n",
    "        norm = {\n",
    "            # NEW fields you asked for\n",
    "            \"label\": q,                           # human label (the mention text)\n",
    "            \"type\": ner_type,                     # FAC/GPE/LOC if present\n",
    "            \"street\": addr.get(\"street\"),\n",
    "            \"city\": addr.get(\"city\"),\n",
    "            \"state\": addr.get(\"state\") or addr.get(\"region\"),\n",
    "            \"country\": addr.get(\"country\"),\n",
    "            \"postcode\": addr.get(\"postcode\"),\n",
    "            \"coordinates\": [lat, lon] if (lat is not None and lon is not None) else None,\n",
    "            \"location_notes\": rec.get(\"display_name\"),\n",
    "\n",
    "            # Existing fields preserved (don’t break later cells)\n",
    "            \"mention\": q,\n",
    "            \"kind\": kind,\n",
    "            \"council\": addr.get(\"council\"),\n",
    "            \"region\": addr.get(\"region\"),\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "            \"display_name\": rec.get(\"display_name\"),\n",
    "            \"geo_source\": rec.get(\"source\"),\n",
    "            \"country_bias\": rec.get(\"country_bias\"),\n",
    "        }\n",
    "\n",
    "        # Stable dedup key (avoid dupes across same resolved point & label/type)\n",
    "        dkey = (\n",
    "            round(lat, 6) if lat is not None else None,\n",
    "            round(lon, 6) if lon is not None else None,\n",
    "            norm[\"label\"], norm[\"type\"], norm[\"street\"], norm[\"city\"], norm[\"state\"], norm[\"country\"]\n",
    "        )\n",
    "        if dkey in seen_keys:\n",
    "            continue\n",
    "        seen_keys.add(dkey)\n",
    "        results.append(norm)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Apply to all rows\n",
    "df[\"resolved_locations\"] = df[\"location_mentions\"].map(geocode_mentions_for_row)\n",
    "\n",
    "# Inspect a few resolved examples\n",
    "cols = [\"title_clean\",\"resolved_locations\"]\n",
    "df[cols].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28774c52",
   "metadata": {},
   "source": [
    "#### 4C. Build `locations_mentioned` (normalized, deduplicated)\n",
    "\n",
    "- Convert each row’s `resolved_locations` (from 4B) into a compact Newswire-ready list.\n",
    "- For every resolved place, construct a record with:\n",
    "  - `label` (mention text), `type` (NER label, e.g., `FAC`/`GPE`/`LOC`)\n",
    "  - `street`, `city`, `state` (falls back to `region`), `country`, optional `postcode`\n",
    "  - `coordinates` as `[lat, lon]` (only if both present)\n",
    "  - `location_notes` (prefer enriched note; fallback to geocoder `display_name`)\n",
    "- **Deduplicate** per article using the key:  \n",
    "  `(label, type, street, city, state, country, round(lat,6), round(lon,6))`  \n",
    "  (Keeps the first occurrence; rounding avoids float-noise duplicates.)\n",
    "- Output column: `df[\"locations_mentioned\"]` — ordered, clean list per article for downst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locations_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'label': 'Max Rayne House', 'type': 'facilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'label': 'Skylab II', 'type': 'facility', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 locations_mentioned\n",
       "0  [{'label': 'Max Rayne House', 'type': 'facilit...\n",
       "1  [{'label': 'Skylab II', 'type': 'facility', 's...\n",
       "2                                                 []"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Build locations_mentioned (one list per article) ----\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def _to_locations_mentioned(resolved_list: List[Dict[str, Any]] | None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Map resolved_locations -> target Newswire schema with enrichments:\n",
    "      {\n",
    "        label, type, street, city, state, country,\n",
    "        coordinates [lat, lon], location_notes, (optional) postcode\n",
    "      }\n",
    "    Deduplicate by (label, type, street, city, state, country, lat, lon).\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    seen = set()\n",
    "    if not isinstance(resolved_list, list):\n",
    "        return out\n",
    "\n",
    "    for r in resolved_list:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        label   = (r.get(\"label\") or r.get(\"mention\") or \"\").strip() or None\n",
    "        typ     = r.get(\"type\") or r.get(\"kind\") or None\n",
    "        street  = r.get(\"street\") or None\n",
    "        city    = r.get(\"city\") or None\n",
    "        state   = r.get(\"state\") or r.get(\"region\") or None\n",
    "        country = r.get(\"country\") or None\n",
    "        postcode = r.get(\"postcode\") or None\n",
    "\n",
    "        lat = r.get(\"lat\"); lon = r.get(\"lon\")\n",
    "        coords = [float(lat), float(lon)] if (lat is not None and lon is not None) else None\n",
    "\n",
    "        notes = r.get(\"location_notes\") or r.get(\"display_name\") or \"\"\n",
    "\n",
    "        # Dedup key\n",
    "        key = (\n",
    "            label, typ, street, city, state, country,\n",
    "            round(float(lat), 6) if lat is not None else None,\n",
    "            round(float(lon), 6) if lon is not None else None,\n",
    "        )\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        rec = {\n",
    "            \"label\": label,\n",
    "            \"type\": typ,\n",
    "            \"street\": street,\n",
    "            \"city\": city,\n",
    "            \"state\": state,\n",
    "            \"country\": country,\n",
    "            \"coordinates\": coords,\n",
    "            \"location_notes\": notes,\n",
    "        }\n",
    "        if postcode is not None:\n",
    "            rec[\"postcode\"] = postcode\n",
    "\n",
    "        out.append(rec)\n",
    "\n",
    "    return out\n",
    "\n",
    "df[\"locations_mentioned\"] = df[\"resolved_locations\"].map(_to_locations_mentioned)\n",
    "\n",
    "# Quick sanity check\n",
    "df[[\"locations_mentioned\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b71eae",
   "metadata": {},
   "source": [
    "#### 5. People NER + disambiguation (context/year-aware)\n",
    "- Collect PERSON mentions (2+ tokens) from NER.\n",
    "- For each mention, link to Wikidata using:\n",
    "  - Search → batch fetch facts (birth/death years, occupations) → score with name similarity.\n",
    "  - Context boost (theme hits in article) and time penalty (publication year).\n",
    "- Cache keyed by mention + context (theme + decade) to avoid repeated lookups.\n",
    "- Save a single `people_mentioned` list: each item has `mention`, `wikidata_id` (or null),\n",
    "\n",
    "RUN TIME ~14 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aff9db00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc9aa9d7b54a9bbdd846c8d93db731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Recent months (DATE) at 0-13\n",
      "Entity: Ifor Evans Hall (ORG) at 45-60\n",
      "Entity: Max Rayne House (FAC) at 65-80\n",
      "Entity: Ifor Evans (FAC) at 361-371\n",
      "Entity: Max Rayne House (FAC) at 376-391\n",
      "Entity: 500 (CARDINAL) at 416-419\n",
      "Entity: next year (DATE) at 499-508\n",
      "Entity: UC (ORG) at 544-546\n",
      "Entity: Ramsey Hall (FAC) at 664-675\n",
      "Entity: only two (CARDINAL) at 792-800\n",
      "Entity: the past five years (DATE) at 832-851\n",
      "Entity: Pi (ORG) at 854-856\n",
      "Entity: Ifor (ORG) at 888-892\n",
      "Entity: Max (ORG) at 897-900\n",
      "Entity: Ifor Evans' (ORG) at 1067-1078\n",
      "Entity: John Andrews (PERSON) at 1091-1103\n",
      "Entity: the day (DATE) at 1204-1211\n",
      "Entity: Pi (PERSON) at 1709-1711\n",
      "Entity: One (CARDINAL) at 1869-1872\n",
      "Entity: a couple of minutes (TIME) at 2064-2083\n",
      "Entity: John Andrews (PERSON) at 2203-2215\n",
      "Entity: Skylab II (FAC) at 0-9\n",
      "Entity: second (ORDINAL) at 15-21\n",
      "Entity: the early '80s (DATE) at 82-96\n",
      "Entity: UCL (ORG) at 111-114\n",
      "Entity: Keith Strong (PERSON) at 275-287\n",
      "Entity: 27 (DATE) at 289-291\n",
      "Entity: the Mullard Space Science Laboratory (ORG) at 319-355\n",
      "Entity: Strong (PERSON) at 393-399\n",
      "Entity: the Mullard Laboratories (ORG) at 420-444\n",
      "Entity: Mill Hill (GPE) at 448-457\n",
      "Entity: UCL (ORG) at 482-485\n",
      "Entity: 1973 (DATE) at 489-493\n",
      "Entity: ten (CARDINAL) at 737-740\n",
      "Entity: the Investigating Workers' Group (ORG) at 771-803\n",
      "Entity: Houston (GPE) at 827-834\n",
      "Entity: ten (CARDINAL) at 845-848\n",
      "Entity: eight (CARDINAL) at 850-855\n",
      "Entity: the Marshall Space Flight Centre (FAC) at 881-913\n",
      "Entity: Huntsville (GPE) at 917-927\n",
      "Entity: Alabama (GPE) at 929-936\n",
      "Entity: IWG (ORG) at 958-961\n",
      "Entity: four (CARDINAL) at 985-989\n",
      "Entity: eight (CARDINAL) at 999-1004\n",
      "Entity: TENTERHOOKS (PERSON) at 1060-1071\n",
      "Entity: Keith Strong (PERSON) at 1075-1087\n",
      "Entity: one (CARDINAL) at 1159-1162\n",
      "Entity: two (CARDINAL) at 1170-1173\n",
      "Entity: Lab (ORG) at 1213-1216\n",
      "Entity: Strong (PERSON) at 1252-1258\n",
      "Entity: Americans (NORP) at 1316-1325\n",
      "Entity: six (CARDINAL) at 1334-1337\n",
      "Entity: this week (DATE) at 1454-1463\n",
      "Entity: UCL (ORG) at 1640-1643\n",
      "Entity: NUS (ORG) at 40-43\n",
      "Entity: King's College (ORG) at 63-77\n",
      "Entity: the beginning of next session (DATE) at 81-110\n",
      "Entity: NUS (ORG) at 188-191\n",
      "Entity: King's (ORG) at 195-201\n",
      "Entity: Tony Atherton (PERSON) at 203-216\n",
      "Entity: Union (ORG) at 267-272\n",
      "Entity: Pi (WORK_OF_ART) at 280-282\n",
      "Entity: King's (ORG) at 345-351\n",
      "Entity: NUS (ORG) at 357-360\n",
      "Entity: next session (DATE) at 464-476\n",
      "Entity: a Universities Association (ORG) at 509-535\n",
      "Entity: NUS (ORG) at 543-546\n",
      "Entity: NUS (ORG) at 623-626\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def ner_bio(text: str):\n",
    "    doc = nlp(text)\n",
    "    words = [t.text for t in doc]\n",
    "    labels = [\"O\"] * len(words)\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Entity: {ent.text} ({ent.label_}) at {ent.start_char}-{ent.end_char}\")\n",
    "        labels[ent.start] = f\"B-{ent.label_}\"\n",
    "        for i in range(ent.start+1, ent.end):\n",
    "            labels[i] = f\"I-{ent.label_}\"\n",
    "    ents = [(e.text, e.label_) for e in doc.ents]\n",
    "    return words, labels, ents\n",
    "\n",
    "# Store compactly; full arrays go to final record\n",
    "df[\"_ner\"] = df[\"text_clean\"].progress_map(ner_bio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ad227",
   "metadata": {},
   "source": [
    "#### 6. Build Newswire-like JSONL\n",
    "- Emit one JSON line per article with normalized fields:\n",
    "  - ids (`issue_id`, `art_id`, `page`, `year`), `title`, `text`\n",
    "  - `locations_mentioned` (enriched objects with `label`, `type`, `street`, `city`, `state`, `country`, `postcode`, `coordinates`, `location_notes`)\n",
    "  - `people_mentioned` (all PERSON mentions; `wikidata_id` null if unresolved)\n",
    "  - `source_file`\n",
    "- Designed for downstream indexing/search and reproducibility.\n",
    "\n",
    "RUN TIME 1.3 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "836ab52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John Andrews']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Step 6: People linking (cache keyed by mention; cache *record* uses canonical 'name') ----\n",
    "import json, re, requests\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "PEOPLE_CACHE = CACHE_DIR / \"people_link_cache.jsonl\"\n",
    "\n",
    "def _norm_key(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "# In-memory cache, looked up by the *mention* text (normalized)\n",
    "_people_store: dict[str, dict | None] = {}\n",
    "\n",
    "# Fresh start is fine if you deleted the cache file; loader is tolerant anyway\n",
    "if PEOPLE_CACHE.exists():\n",
    "    with open(PEOPLE_CACHE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not isinstance(rec, dict):\n",
    "                continue\n",
    "            # Prefer original mention to be the key if available; fall back safely\n",
    "            key = rec.get(\"query\") or rec.get(\"name\") or rec.get(\"person_name\")\n",
    "            if key:\n",
    "                _people_store[_norm_key(key)] = rec\n",
    "\n",
    "def _cache_person_write(mention: str, resolved: dict | None):\n",
    "    \"\"\"\n",
    "    Write one JSONL record per mention:\n",
    "      - If resolved: include canonical 'name' (Wikidata label) and the original 'query' (mention).\n",
    "      - If unresolved: write a minimal record with 'name': None.\n",
    "    \"\"\"\n",
    "    PEOPLE_CACHE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    key = _norm_key(mention)\n",
    "\n",
    "    if resolved is None:\n",
    "        rec = {\"query\": mention, \"name\": None}\n",
    "        _people_store[key] = rec\n",
    "    else:\n",
    "        # Ensure canonical 'name' present; keep mention in 'query'\n",
    "        canon = resolved.get(\"person_name\") or resolved.get(\"name\") or mention\n",
    "        rec = {\n",
    "            \"query\": mention,\n",
    "            \"name\": canon,                 # canonical label\n",
    "            **resolved\n",
    "        }\n",
    "        _people_store[key] = rec\n",
    "\n",
    "    with open(PEOPLE_CACHE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def wb_search(name: str, limit=5):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"language\": \"en\",\n",
    "        \"uselang\": \"en\",\n",
    "        \"type\": \"item\",\n",
    "        \"search\": name,\n",
    "        \"limit\": limit,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers={\"User-Agent\": \"pi-newswire-pipeline\"})\n",
    "    if not r.ok:\n",
    "        return []\n",
    "    return r.json().get(\"search\", [])\n",
    "\n",
    "def is_human_qid(qid: str) -> bool:\n",
    "    # SPARQL: instance of human (Q5)\n",
    "    sparql = f\"\"\"\n",
    "    ASK {{ wd:{qid} wdt:P31 wd:Q5 . }}\n",
    "    \"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    r = requests.get(url, params={\"format\": \"json\", \"query\": sparql}, headers={\"User-Agent\": \"pi-newswire-pipeline\"})\n",
    "    if not r.ok:\n",
    "        return False\n",
    "    try:\n",
    "        return bool(r.json().get(\"boolean\", False))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def link_person(mention: str, context: str | None = None):\n",
    "    \"\"\"\n",
    "    Resolve a PERSON mention to Wikidata.\n",
    "    Cache lookup is by *mention* (normalized).\n",
    "    Cache record stores the canonical 'name' (Wikidata label) and keeps 'query'=mention.\n",
    "    Returns a dict like {\"wikidata_id\": \"Qxx\", \"person_name\": \"...\"} or None.\n",
    "    \"\"\"\n",
    "    key = _norm_key(mention)\n",
    "    if key in _people_store:\n",
    "        return _people_store[key] if _people_store[key].get(\"name\") else None\n",
    "\n",
    "    cands = wb_search(mention, limit=7)\n",
    "    best = None\n",
    "    best_score = -1\n",
    "\n",
    "    for c in cands:\n",
    "        qid = c.get(\"id\")\n",
    "        if not qid:\n",
    "            continue\n",
    "        label = c.get(\"label\") or \"\"\n",
    "        desc  = c.get(\"description\") or \"\"\n",
    "        score = fuzz.token_set_ratio(mention, label)\n",
    "        if score > best_score:\n",
    "            best = {\"wikidata_id\": qid, \"person_name\": label, \"desc\": desc}\n",
    "            best_score = score\n",
    "\n",
    "    # Validate human & write cache\n",
    "    if best and best.get(\"wikidata_id\") and is_human_qid(best[\"wikidata_id\"]):\n",
    "        out = {\n",
    "            \"wikidata_id\": best[\"wikidata_id\"],\n",
    "            \"person_name\": best[\"person_name\"]\n",
    "            # Optionally enrich later: gender, occupations, etc.\n",
    "        }\n",
    "        _cache_person_write(mention, out)\n",
    "        return out\n",
    "\n",
    "    _cache_person_write(mention, None)\n",
    "    return None\n",
    "\n",
    "def people_from_ner(ents):\n",
    "    # Two-token+ names only (avoid bare surnames); adjust if you want\n",
    "    return sorted({text for (text, label) in ents if label == \"PERSON\" and len(text.split()) >= 2})\n",
    "\n",
    "# Quick smoke test on first row:\n",
    "sample_words, sample_labels, sample_ents = df[\"_ner\"].iloc[0]\n",
    "people = people_from_ner(sample_ents)\n",
    "people[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2f72e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous version of transformers did not work. The suggested solution was to update it. \n",
    "# %pip install -U \"transformers>=4.41\" \"huggingface_hub>=0.23\" \"safetensors>=0.4.3\" \"torch>=2.1,<3\"\n",
    "# CHECK again BEFORE USING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9c718",
   "metadata": {},
   "source": [
    "##### Installing Topic Classification Models Locally\n",
    "\n",
    "This section downloads pre-trained topic classification models from Dell Research Harvard for newspaper content analysis. These models are specifically trained to identify different topics in news articles.\n",
    "\n",
    "###### Prerequisites\n",
    "\n",
    "Run these commands in your terminal (outside of Jupyter) to set up the Hugging Face CLI and download the models:\n",
    "\n",
    "###### Step 1: Install Hugging Face CLI\n",
    "\n",
    "```bash\n",
    "pip install -U \"huggingface_hub[cli]\" hf_transfer\n",
    "```\n",
    "\n",
    "###### Step 2: Enable Faster Downloads (Optional)\n",
    "\n",
    "```bash\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "```\n",
    "\n",
    "###### Step 3: Create Local Model Directory\n",
    "\n",
    "```bash\n",
    "MODEL_HOME=\"$HOME/hf-models/newswire\"\n",
    "mkdir -p \"$MODEL_HOME\"\n",
    "```\n",
    "\n",
    "###### Step 4: Download Topic Classification Models\n",
    "\n",
    "```bash\n",
    "for m in \\\n",
    "  dell-research-harvard/topic-antitrust \\\n",
    "  dell-research-harvard/topic-civil_rights \\\n",
    "  dell-research-harvard/topic-crime \\\n",
    "  dell-research-harvard/topic-govt_regulation \\\n",
    "  dell-research-harvard/topic-labor_movement \\\n",
    "  dell-research-harvard/topic-politics \\\n",
    "  dell-research-harvard/topic-protests \\\n",
    "  dell-research-harvard/topic-sport \\\n",
    "  dell-research-harvard/topic-fire \\\n",
    "  dell-research-harvard/topic-weather \\\n",
    "  dell-research-harvard/topic-obits\n",
    "do\n",
    "  name=\"${m##*/}\"\n",
    "  huggingface-cli download \"$m\" --local-dir \"$MODEL_HOME/$name\" --local-dir-use-symlinks False\n",
    "done\n",
    "```\n",
    "\n",
    "###### Notes\n",
    "\n",
    "- Models will be saved to `~/hf-models/newswire/` by default\n",
    "- Each model is approximately 400-500MB\n",
    "- Download time depends on your internet connection\n",
    "- Models are stored without symlinks for better portability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef7708",
   "metadata": {},
   "source": [
    "#### 7. Topic tagging with ALL Newswire heads (local-first, safe, cached) ----\n",
    "\n",
    "RUN TIME ~long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "005135e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "antitrust          0\n",
       "civil_rights       0\n",
       "crime              0\n",
       "govt_regulation    0\n",
       "labor_movement     1\n",
       "politics           0\n",
       "protests           0\n",
       "sport              0\n",
       "fire               0\n",
       "weather            0\n",
       "obits              0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Step 7: Topic tagging with ALL Newswire heads (local-first, safe, cached) ----\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import json, time\n",
    "\n",
    "# 0) Ensure text column is strings\n",
    "df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\").astype(str)\n",
    "\n",
    "# 1) Point to where you stored the models (EDIT THIS to your path if different)\n",
    "MODEL_HOME = Path(\"newswire-topic-models\")\n",
    "\n",
    "# 2) All Newswire topic heads\n",
    "TOPIC_MODELS = {\n",
    "    \"antitrust\":        \"dell-research-harvard/topic-antitrust\",\n",
    "    \"civil_rights\":     \"dell-research-harvard/topic-civil_rights\",\n",
    "    \"crime\":            \"dell-research-harvard/topic-crime\",\n",
    "    \"govt_regulation\":  \"dell-research-harvard/topic-govt_regulation\",\n",
    "    \"labor_movement\":   \"dell-research-harvard/topic-labor_movement\",\n",
    "    \"politics\":         \"dell-research-harvard/topic-politics\",\n",
    "    \"protests\":         \"dell-research-harvard/topic-protests\",\n",
    "    \"sport\":            \"dell-research-harvard/topic-sport\",\n",
    "    \"fire\":             \"dell-research-harvard/topic-fire\",\n",
    "    \"weather\":          \"dell-research-harvard/topic-weather\",\n",
    "    \"obits\":            \"dell-research-harvard/topic-obits\",\n",
    "}\n",
    "\n",
    "# Helper: prefer local folder if present\n",
    "def resolve_model_id(mid: str) -> str:\n",
    "    local = MODEL_HOME / mid.split(\"/\", 1)[1]\n",
    "    return str(local) if local.exists() else mid\n",
    "\n",
    "# If you want to force strictly-offline (error if model missing locally), set:\n",
    "STRICT_OFFLINE = False  # True -> raise if local folder missing\n",
    "\n",
    "DEVICE    = -1      # -1 = CPU; set 0 for CUDA-GPU, or None to auto-pick (MPS on Apple Silicon)\n",
    "BATCH_SIZE= 8\n",
    "TRUNC     = 4000\n",
    "THRESH    = 0.5\n",
    "\n",
    "_topic_pipes = {}\n",
    "def get_pipe(model_id: str):\n",
    "    model_ref = resolve_model_id(model_id)\n",
    "    if STRICT_OFFLINE and not Path(model_ref).exists():\n",
    "        raise FileNotFoundError(f\"Local model not found: {model_ref}\")\n",
    "    if model_ref in _topic_pipes:\n",
    "        return _topic_pipes[model_ref]\n",
    "    try:\n",
    "        _topic_pipes[model_ref] = pipeline(\"text-classification\", model=model_ref, device=DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not load {model_ref}: {type(e).__name__}: {e}\")\n",
    "        _topic_pipes[model_ref] = None\n",
    "    return _topic_pipes[model_ref]\n",
    "\n",
    "def _to_binary_label(hf_output, threshold=THRESH):\n",
    "    if not hf_output:\n",
    "        return 0\n",
    "    rec = hf_output[0] if isinstance(hf_output, list) else hf_output\n",
    "    label = str(rec.get(\"label\",\"\")).upper()\n",
    "    score = float(rec.get(\"score\", 0.0))\n",
    "    is_pos = (\"1\" in label) or (\"POS\" in label) or (\"YES\" in label)\n",
    "    return 1 if (is_pos and score >= threshold) else 0\n",
    "\n",
    "def score_one_model(model_id: str, texts: list[str]) -> list[int]:\n",
    "    clf = get_pipe(model_id)\n",
    "    if clf is None:\n",
    "        return [0]*len(texts)\n",
    "    # retry once on transient errors\n",
    "    try:\n",
    "        preds = clf([ (t or \"\")[:TRUNC] for t in texts ], batch_size=BATCH_SIZE, truncation=True)\n",
    "    except Exception:\n",
    "        time.sleep(1.0)\n",
    "        preds = clf([ (t or \"\")[:TRUNC] for t in texts ], batch_size=BATCH_SIZE, truncation=True)\n",
    "    return [_to_binary_label(p) for p in preds]\n",
    "\n",
    "# Cache predictions so re-runs are quick\n",
    "CACHE_PATH = Path(OUTPUT_ROOT) / \"cache\" / \"topic_preds.jsonl\"\n",
    "CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "_cache = {}\n",
    "if CACHE_PATH.exists():\n",
    "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            _cache[(j[\"source_file\"], int(j[\"unit_index\"]))] = j[\"topics\"]\n",
    "\n",
    "def _write_cache_row(key, topics_dict):\n",
    "    with open(CACHE_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"source_file\": key[0], \"unit_index\": key[1], \"topics\": topics_dict}) + \"\\n\")\n",
    "\n",
    "# Mini-batch over rows to reduce per-model calls\n",
    "B = 32\n",
    "pending_keys, pending_texts = [], []\n",
    "topics_all = []\n",
    "\n",
    "def flush_pending():\n",
    "    global pending_keys, pending_texts, topics_all\n",
    "    if not pending_texts:\n",
    "        return\n",
    "    batch = {t: score_one_model(mid, pending_texts) for t, mid in TOPIC_MODELS.items()}\n",
    "    for i, key in enumerate(pending_keys):\n",
    "        row_topics = {t: int(batch[t][i]) for t in TOPIC_MODELS}\n",
    "        topics_all.append(row_topics)\n",
    "        _write_cache_row(key, row_topics)\n",
    "    pending_keys.clear(); pending_texts.clear()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    key = (row[\"source_file\"], int(row[\"unit_index\"]))\n",
    "    if key in _cache:\n",
    "        topics_all.append(_cache[key])\n",
    "        continue\n",
    "    pending_keys.append(key); pending_texts.append(row[\"text_clean\"])\n",
    "    if len(pending_texts) >= B:\n",
    "        flush_pending()\n",
    "flush_pending()\n",
    "\n",
    "df[\"topics\"] = topics_all\n",
    "for t in TOPIC_MODELS:\n",
    "    df[t] = df[\"topics\"].map(lambda d: int(d.get(t, 0)))\n",
    "\n",
    "# Quick tally so you can see it worked\n",
    "display(df[[*TOPIC_MODELS.keys()]].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3915049",
   "metadata": {},
   "source": [
    "#### 8. Build Newswire-like records & export Issue-bucketed JSONL\n",
    "- Read the Newswire JSONL and extract a tidy table:\n",
    "  - `year`, `source_file`, `issue`, `art_id`, `article`, `page`\n",
    "  - `label`, `type`, `street`, `city`, `state`, `country`, `postcode`\n",
    "  - `longitude`, `latitude`, `location_notes`\n",
    "- Save to `locations_mentioned.csv` for spreadsheet/curation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4ea0b6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351b58f9604448bcb7f5f0ccb13a7a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing 1979:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/2-semantic-processing/output-semantic/pi_newswire_like/1979_pi_newswire_like.jsonl\n"
     ]
    }
   ],
   "source": [
    "def to_newswire_like(row):\n",
    "    words, labels, ents = row[\"_ner\"]\n",
    "    ppl   = people_from_ner(ents)\n",
    "    linked = [link_person(p, row[\"text_clean\"]) for p in ppl]\n",
    "    linked = [x for x in linked if x]\n",
    "\n",
    "    return {\n",
    "        \"year\": int(row[\"year\"]),\n",
    "        \"dates\": [],\n",
    "        \"article\": row[\"text_clean\"],\n",
    "        \"byline\": \"\",\n",
    "        \"newspaper_metadata\": [],\n",
    "        \"source_file\": row.get(\"source_file\"),  # full path or filename from Step 2\n",
    "        \"art_id\": row.get(\"id\"),                # map DF 'id' -> JSONL 'art_id'\n",
    "        \"page\"\n",
    "\n",
    "        # topics (present if you ran Step 7)\n",
    "        \"antitrust\":        int(row.get(\"antitrust\", 0)),\n",
    "        \"civil_rights\":     int(row.get(\"civil_rights\", 0)),\n",
    "        \"crime\":            int(row.get(\"crime\", 0)),\n",
    "        \"govt_regulation\":  int(row.get(\"govt_regulation\", 0)),\n",
    "        \"labor_movement\":   int(row.get(\"labor_movement\", 0)),\n",
    "        \"politics\":         int(row.get(\"politics\", 0)),\n",
    "        \"protests\":         int(row.get(\"protests\", 0)),\n",
    "        \"sport\":            int(row.get(\"sport\", 0)),\n",
    "        \"fire\":             int(row.get(\"fire\", 0)),\n",
    "        \"weather\":          int(row.get(\"weather\", 0)),\n",
    "        \"obits\":            int(row.get(\"obits\", 0)),\n",
    "\n",
    "        # NER (BIO)\n",
    "        \"ner_words\":  words,\n",
    "        \"ner_labels\": labels,\n",
    "\n",
    "        # New: all resolved locations\n",
    "        \"locations_mentioned\": row.get(\"locations_mentioned\", []),\n",
    "\n",
    "        # Optional: keep for audit (not required in the final JSON)\n",
    "        # \"resolved_locations\": row.get(\"resolved_locations\", []),\n",
    "\n",
    "        # People\n",
    "        \"people_mentioned\": linked,\n",
    "    }\n",
    "\n",
    "\n",
    "out_dir = Path(OUTPUT_ROOT) / \"pi_newswire_like\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_year_buckets(frame: pd.DataFrame):\n",
    "    for y, g in frame.groupby(\"year\"):\n",
    "        outp = out_dir / f\"{y}_pi_newswire_like.jsonl\"\n",
    "        with open(outp, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in tqdm(g.iterrows(), total=len(g), desc=f\"Writing {y}\"):\n",
    "                f.write(json.dumps(to_newswire_like(row), ensure_ascii=False) + \"\\n\")\n",
    "        print(\"Wrote\", outp)\n",
    "\n",
    "write_year_buckets(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4bf59eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13610307090248e7871ecba884ce2bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Pi-Newspaper-1979-cleaned.jsonl:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/2-semantic-processing/output-semantic/newswire-format-issues/1979/Pi-Newspaper-1979-cleaned.jsonl\n"
     ]
    }
   ],
   "source": [
    "# --- Step 8B: Export issue-based JSONL (preserve year folders + filename) ---\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir_issue = OUTPUT_ROOT / \"newswire-format-issues\"\n",
    "out_dir_issue.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_newswire_like(row):\n",
    "    words, labels, ents = row[\"_ner\"]\n",
    "    ppl   = people_from_ner(ents)\n",
    "    linked = [link_person(p, row[\"text_clean\"]) for p in ppl]\n",
    "    linked = [x for x in linked if x]\n",
    "\n",
    "    return {\n",
    "        \"year\": int(row[\"year\"]),\n",
    "        \"dates\": [],\n",
    "        \"article\": row[\"text_clean\"],\n",
    "        \"byline\": \"\",\n",
    "        \"newspaper_metadata\": [],\n",
    "        \"source_file\": row.get(\"source_file\"),  # full path or filename from Step 2\n",
    "        \"art_id\": row.get(\"id\"),                # map DF 'id' -> JSONL 'art_id'\n",
    "        \"page\"\n",
    "\n",
    "        # topics (present if you ran Step 7)\n",
    "        \"antitrust\":        int(row.get(\"antitrust\", 0)),\n",
    "        \"civil_rights\":     int(row.get(\"civil_rights\", 0)),\n",
    "        \"crime\":            int(row.get(\"crime\", 0)),\n",
    "        \"govt_regulation\":  int(row.get(\"govt_regulation\", 0)),\n",
    "        \"labor_movement\":   int(row.get(\"labor_movement\", 0)),\n",
    "        \"politics\":         int(row.get(\"politics\", 0)),\n",
    "        \"protests\":         int(row.get(\"protests\", 0)),\n",
    "        \"sport\":            int(row.get(\"sport\", 0)),\n",
    "        \"fire\":             int(row.get(\"fire\", 0)),\n",
    "        \"weather\":          int(row.get(\"weather\", 0)),\n",
    "        \"obits\":            int(row.get(\"obits\", 0)),\n",
    "\n",
    "        # NER (BIO)\n",
    "        \"ner_words\":  words,\n",
    "        \"ner_labels\": labels,\n",
    "\n",
    "        # New: all resolved locations\n",
    "        \"locations_mentioned\": row.get(\"locations_mentioned\", []),\n",
    "\n",
    "        # Optional: keep for audit (not required in the final JSON)\n",
    "        # \"resolved_locations\": row.get(\"resolved_locations\", []),\n",
    "\n",
    "        # People\n",
    "        \"people_mentioned\": linked,\n",
    "    }\n",
    "\n",
    "def write_issue_buckets(frame: pd.DataFrame):\n",
    "    # Keep stable ordering within each issue\n",
    "    sort_cols = [c for c in [\"year\", \"source_file\", \"unit_index\"] if c in frame.columns]\n",
    "    gframe = frame.sort_values(sort_cols, kind=\"stable\")\n",
    "\n",
    "    # Group by issue (source_file) within each year\n",
    "    for (y, src), g in gframe.groupby([\"year\", \"source_file\"], sort=False):\n",
    "        year_dir = out_dir_issue / str(int(y))\n",
    "        year_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Keep original filename but change extension to .jsonl\n",
    "        fname = Path(src).with_suffix(\".jsonl\").name\n",
    "        outp = year_dir / fname\n",
    "\n",
    "        with open(outp, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in tqdm(g.iterrows(), total=len(g), desc=f\"Writing {fname}\"):\n",
    "                f.write(json.dumps(to_newswire_like(row), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(\"Wrote\", outp)\n",
    "\n",
    "write_issue_buckets(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d246edb",
   "metadata": {},
   "source": [
    "#### 9. Per-issue CSV summary with Topics, NERs, Geolocations. (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3a8736e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Helper: issue id from filename --------\n",
    "ISSUE_RX = re.compile(r\"^pi_(.+?)_(19|20)\\d\\d$\")  # captures between 'pi_' and '_YYYY'\n",
    "def issue_id_from_path(p: str) -> str:\n",
    "    stem = Path(p).stem  # e.g., 'pi_vol_2_3_1948-cleaned' -> 'pi_vol_2_3_1948-cleaned'\n",
    "    stem = stem.replace(\"-cleaned\",\"\")\n",
    "    m = ISSUE_RX.match(stem)\n",
    "    if m:\n",
    "        return m.group(1)  # e.g., '1', 'vol_2_3', '19_first_edition', 'rag'\n",
    "    # fallback: remove prefix 'pi_' and year suffix\n",
    "    s = stem\n",
    "    if s.startswith(\"pi_\"): s = s[3:]\n",
    "    s = re.sub(r\"_(19|20)\\d\\d$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "df[\"issue_id\"] = df[\"source_file\"].map(issue_id_from_path)\n",
    "\n",
    "# -------- Topics per issue --------\n",
    "topic_cols_all = [\n",
    "    \"antitrust\",\"civil_rights\",\"crime\",\"govt_regulation\",\"labor_movement\",\n",
    "    \"politics\",\"protests\",\"sport\",\"fire\",\"weather\",\"obits\"\n",
    "]\n",
    "topic_cols = [c for c in topic_cols_all if c in df.columns]\n",
    "\n",
    "def topics_for_group(g: pd.DataFrame):\n",
    "    present = [t for t in topic_cols if g[t].sum() > 0]\n",
    "    return present, len(present)\n",
    "\n",
    "# -------- NERs per issue (top 25 by frequency) --------\n",
    "ALLOWED_NER = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"FAC\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"NORP\"}\n",
    "def ners_for_group(g: pd.DataFrame, topk=25):\n",
    "    cnt = Counter()\n",
    "    for item in g[\"_ner\"]:\n",
    "        if not isinstance(item, (list, tuple)) or len(item) < 3:\n",
    "            continue\n",
    "        _, _, ents = item\n",
    "        for text, label in ents:\n",
    "            if label in ALLOWED_NER:\n",
    "                norm = text.strip().strip('.,;:\"\\'()[]')\n",
    "                if norm:\n",
    "                    cnt[norm] += 1\n",
    "    n_unique = len(cnt)\n",
    "    listed = [f\"{k} ({v})\" for k,v in cnt.most_common(topk)]\n",
    "    return listed, n_unique\n",
    "\n",
    "# -------- Geolocations per issue (top 20) --------\n",
    "def geos_for_group(g: pd.DataFrame, topk=20):\n",
    "    cnt = Counter()\n",
    "    for L in g.get(\"resolved_locations\", []):\n",
    "        if not isinstance(L, list):\n",
    "            continue\n",
    "        for r in L:\n",
    "            parts = [r.get(\"street\"), r.get(\"council\"), r.get(\"city\"), r.get(\"region\"), r.get(\"country\")]\n",
    "            name = \", \".join([p for p in parts if p])\n",
    "            if not name:\n",
    "                name = r.get(\"display_name\") or \"\"\n",
    "            name = name.strip().strip(\",\")\n",
    "            if name:\n",
    "                cnt[name] += 1\n",
    "    n_unique = len(cnt)\n",
    "    listed = [f\"{k} ({v})\" for k,v in cnt.most_common(topk)]\n",
    "    return listed, n_unique\n",
    "\n",
    "# -------- People Mentioned per issue (top 20) --------\n",
    "def people_for_group(g: pd.DataFrame, topk=20):\n",
    "    cnt = Counter()\n",
    "\n",
    "    # Prefer the explicit column if it exists; otherwise derive from NER\n",
    "    if \"people_mentioned\" in g.columns:\n",
    "        series = g[\"people_mentioned\"]\n",
    "    else:\n",
    "        # Fallback: derive from NER spans (expects people_from_ner and _ner available)\n",
    "        def _from_ner(item):\n",
    "            if not isinstance(item, (list, tuple)) or len(item) < 3:\n",
    "                return []\n",
    "            _, _, ents = item\n",
    "            try:\n",
    "                names = people_from_ner(ents)  # returns list of strings\n",
    "            except NameError:\n",
    "                names = [text for (text, label) in ents if label == \"PERSON\" and len(text.split()) >= 2]\n",
    "            return [{\"person_name\": n} for n in names]\n",
    "        series = g[\"_ner\"].map(_from_ner) if \"_ner\" in g.columns else pd.Series([[]]*len(g), index=g.index)\n",
    "\n",
    "    for L in series:\n",
    "        if not isinstance(L, list):\n",
    "            continue\n",
    "        for r in L:\n",
    "            # r may be a dict from linker, or a bare string if upstream changed\n",
    "            if isinstance(r, dict):\n",
    "                name = r.get(\"person_name\") or r.get(\"name\") or \"\"\n",
    "            else:\n",
    "                name = str(r) if r is not None else \"\"\n",
    "            name = re.sub(r\"\\s+\", \" \", name).strip().strip('.,;:\"\\'()[]')\n",
    "            if name:\n",
    "                cnt[name] += 1\n",
    "\n",
    "    n_unique = len(cnt)\n",
    "    listed = [f\"{k} ({v})\" for k, v in cnt.most_common(topk)]\n",
    "    return listed, n_unique\n",
    "    \n",
    "\n",
    "# -------- Build summary table --------\n",
    "rows = []\n",
    "for (year, issue), g in df.groupby([\"year\",\"issue_id\"], sort=True):\n",
    "    topics_list, topics_n = topics_for_group(g)\n",
    "    ners_list, ners_n     = ners_for_group(g)\n",
    "    geos_list, geos_n     = geos_for_group(g)\n",
    "    people_list, people_n = people_for_group(g)\n",
    "    rows.append({\n",
    "        \"Year\": year,\n",
    "        \"Issue\": issue,\n",
    "        \"List of Topics\": \", \".join(topics_list),\n",
    "        \"Number of Topics\": topics_n,\n",
    "        \"List of NERs (top 25)\": \"; \".join(ners_list),\n",
    "        \"Number of NERs (unique)\": ners_n,\n",
    "        \"List of Geolocations (top 20)\": \"; \".join(geos_list),\n",
    "        \"Number of Geolocations (unique)\": geos_n,\n",
    "        \"List of People Mentioned (top 20)\": \"; \".join(people_list),\n",
    "        \"Number of People Mentioned (unique)\": people_n,\n",
    "        \"Articles in Issue\": len(g),\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).sort_values([\"Year\",\"Issue\"]).reset_index(drop=True)\n",
    "summary\n",
    "summary.to_csv(\"pi_newswire_output_summary.csv\", index=False, encoding=\"utf-8\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37207d0",
   "metadata": {},
   "source": [
    "#### 10. Convert JSONL output to CSV for Wikidata upload\n",
    "##### 10 a. Extract People Mentioned\n",
    "This section iterates through the files in outpu-semantic->newswire-format-issues and creates a single file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 rows to /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/2-semantic-processing/output-semantic/csv-exports/locations_mentioned.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 9: Extract people from JSONL into a CSV (basename + issue) ---\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "people_rows = []\n",
    "\n",
    "# Path to Step 8 JSONL exports\n",
    "jsonl_dir = Path(OUTPUT_ROOT) / \"newswire-format-issues\"\n",
    "CSV_DIR   =  OUTPUT_ROOT/\"csv-exports\"\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_csv   = Path(CSV_DIR) / \"people_mentioned.csv\"\n",
    "\n",
    "\n",
    "def make_issue(name: str | None) -> str | None:\n",
    "    \"\"\"From a filename (no dirs), strip '-cleaned.jsonl' (preferred), else '-cleaned.json',\n",
    "    else just remove .jsonl/.json. Return None if name is falsy.\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    s = name\n",
    "    for suf in (\"-cleaned.jsonl\", \"-cleaned.json\", \".jsonl\", \".json\"):\n",
    "        if s.endswith(suf):\n",
    "            s = s[: -len(suf)]\n",
    "            break\n",
    "    return s\n",
    "\n",
    "for f in sorted(jsonl_dir.glob(\"*.jsonl\")):\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "\n",
    "            year        = rec.get(\"year\")\n",
    "            sf_full     = rec.get(\"source_file\")\n",
    "            sf_name     = Path(sf_full).name if sf_full else None  # [1] basename only\n",
    "            issue_name  = make_issue(sf_name)                      # [2] derived issue\n",
    "\n",
    "            art_id      = rec.get(\"art_id\")\n",
    "            page        = rec.get(\"page\")\n",
    "\n",
    "            # Expecting a list of dicts in \"people_mentioned\"\n",
    "            for p in rec.get(\"people_mentioned\", []):\n",
    "                # Defensive parsing: handle dicts or plain strings\n",
    "                if isinstance(p, dict):\n",
    "                    name        = p.get(\"name\")\n",
    "                    wikidata_id = p.get(\"wikidata_id\")\n",
    "                    person_name = p.get(\"person_name\", name)\n",
    "                else:\n",
    "                    # fallback if it's just a string\n",
    "                    name        = str(p)\n",
    "                    wikidata_id = None\n",
    "                    person_name = str(p)\n",
    "\n",
    "                people_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"source_file\": sf_name,  # basename only\n",
    "                    \"issue\": issue_name,     # new column\n",
    "                    \"art_id\": art_id,\n",
    "                    \"page\": page,\n",
    "                    \"name\": name,\n",
    "                    \"wikidata_id\": wikidata_id,\n",
    "                    \"person_name\": person_name,\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "people_df = pd.DataFrame(people_rows)\n",
    "\n",
    "# Save to CSV\n",
    "people_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(people_df)} rows to {out_csv}\")\n",
    "\n",
    "people_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69cf85b",
   "metadata": {},
   "source": [
    "#### 10. Convert JSONL output to CSV for Wikidata upload\n",
    "##### 10 b. Extract Places Mentioned\n",
    "This section iterates through the files in output-semantic->csv-\n",
    "exports and creates a single file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 rows to /Users/stepanyan/Documents/UCL/GitHub-Projects/Newspaper-Semantic-Enrichment/2-semantic-processing/output-semantic/csv-exports/locations_mentioned.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 10: Extract locations from JSONL into a CSV (with components) ---\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Inputs/outputs\n",
    "jsonl_dir = Path(OUTPUT_ROOT) / \"newswire-format-issues\"\n",
    "CSV_DIR   =  OUTPUT_ROOT/\"csv-exports\"\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_csv   = Path(CSV_DIR) / \"locations_mentioned.csv\"\n",
    "\n",
    "# ---- Helpers\n",
    "def make_issue(name: str | None) -> str | None:\n",
    "    \"\"\"From a filename (no dirs), strip '-cleaned.jsonl' (preferred), else '-cleaned.json',\n",
    "    else just remove .jsonl/.json. Return None if name is falsy.\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    s = Path(name).name\n",
    "    for suf in (\"-cleaned.jsonl\", \"-cleaned.json\", \".jsonl\", \".json\"):\n",
    "        if s.endswith(suf):\n",
    "            return s[: -len(suf)]\n",
    "    return Path(s).stem\n",
    "\n",
    "def to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_location(loc) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize a location entry (dict or str) to:\n",
    "    label, type, street, city, state, country, postcode, longitude, latitude, location_notes\n",
    "    Handles our structured output (with 'coordinates') and common Nominatim shapes.\n",
    "    \"\"\"\n",
    "    # Defaults\n",
    "    out = {\n",
    "        \"label\": None,\n",
    "        \"type\": None,\n",
    "        \"street\": None,\n",
    "        \"city\": None,\n",
    "        \"state\": None,\n",
    "        \"country\": None,\n",
    "        \"postcode\": None,\n",
    "        \"longitude\": None,\n",
    "        \"latitude\": None,\n",
    "        \"location_notes\": None,\n",
    "    }\n",
    "\n",
    "    # If it's a plain string, store it as a note\n",
    "    if not isinstance(loc, dict):\n",
    "        out[\"location_notes\"] = str(loc)\n",
    "        return out\n",
    "\n",
    "    d = loc\n",
    "\n",
    "    # 0) New enriched fields from upstream\n",
    "    out[\"label\"]   = (d.get(\"label\") or d.get(\"mention\") or None)\n",
    "    out[\"type\"]    = (d.get(\"type\")  or d.get(\"kind\")    or None)\n",
    "    out[\"street\"]  = d.get(\"street\")\n",
    "    out[\"postcode\"] = d.get(\"postcode\")\n",
    "\n",
    "    # 1) Direct city/state/country fields (with fallbacks)\n",
    "    out[\"city\"]    = d.get(\"city\")    or d.get(\"town\") or d.get(\"village\") or d.get(\"municipality\")\n",
    "    out[\"state\"]   = d.get(\"state\")   or d.get(\"region\") or d.get(\"province\")\n",
    "    out[\"country\"] = d.get(\"country\")\n",
    "\n",
    "    # 2) Coordinates:\n",
    "    # Prefer unified 'coordinates' = [lat, lon] if present, else fall back to lat/lon keys.\n",
    "    coords = d.get(\"coordinates\")\n",
    "    if isinstance(coords, (list, tuple)) and len(coords) >= 2:\n",
    "        out[\"latitude\"]  = to_float(coords[0])\n",
    "        out[\"longitude\"] = to_float(coords[1])\n",
    "    else:\n",
    "        lon = d.get(\"longitude\", d.get(\"lon\", d.get(\"lng\")))\n",
    "        lat = d.get(\"latitude\",  d.get(\"lat\"))\n",
    "        # GeoJSON-ish geometry fallback\n",
    "        if lon is None or lat is None:\n",
    "            geom = d.get(\"geometry\") or d.get(\"geo\") or d.get(\"point\")\n",
    "            if isinstance(geom, dict):\n",
    "                if \"coordinates\" in geom and isinstance(geom[\"coordinates\"], (list, tuple)) and len(geom[\"coordinates\"]) >= 2:\n",
    "                    lon = geom[\"coordinates\"][0]\n",
    "                    lat = geom[\"coordinates\"][1]\n",
    "                else:\n",
    "                    lon = lon or geom.get(\"lon\") or geom.get(\"lng\")\n",
    "                    lat = lat or geom.get(\"lat\")\n",
    "        out[\"longitude\"] = to_float(lon)\n",
    "        out[\"latitude\"]  = to_float(lat)\n",
    "\n",
    "    # 3) Nominatim-style address fallback (fills missing street/city/state/country/postcode)\n",
    "    addr = d.get(\"address\")\n",
    "    if isinstance(addr, dict):\n",
    "        out[\"street\"]   = out[\"street\"]   or addr.get(\"road\") or addr.get(\"pedestrian\") or addr.get(\"residential\") or addr.get(\"street\")\n",
    "        out[\"city\"]     = out[\"city\"]     or addr.get(\"city\") or addr.get(\"town\") or addr.get(\"village\") or addr.get(\"hamlet\") or addr.get(\"suburb\")\n",
    "        out[\"state\"]    = out[\"state\"]    or addr.get(\"state\") or addr.get(\"region\") or addr.get(\"province\")\n",
    "        out[\"country\"]  = out[\"country\"]  or addr.get(\"country\")\n",
    "        out[\"postcode\"] = out[\"postcode\"] or addr.get(\"postcode\")\n",
    "\n",
    "    # 4) Notes / display name / provenance\n",
    "    # Prefer explicit 'location_notes' if present; otherwise collect a few fields.\n",
    "    if d.get(\"location_notes\"):\n",
    "        out[\"location_notes\"] = d[\"location_notes\"]\n",
    "    else:\n",
    "        parts = []\n",
    "        for key in (\"display_name\", \"source\", \"confidence\"):\n",
    "            v = d.get(key)\n",
    "            if v is not None and str(v).strip():\n",
    "                parts.append(f\"{key}={v}\")\n",
    "        out[\"location_notes\"] = \"; \".join(parts) if parts else None\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---- Main extraction\n",
    "rows = []\n",
    "\n",
    "for f in sorted(jsonl_dir.glob(\"*.jsonl\")):\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "\n",
    "            year        = rec.get(\"year\")\n",
    "            sf_full     = rec.get(\"source_file\")\n",
    "            sf_name     = Path(sf_full).name if sf_full else None\n",
    "            issue_name  = make_issue(sf_name)\n",
    "\n",
    "            art_id      = rec.get(\"art_id\")\n",
    "            page        = rec.get(\"page\")\n",
    "            article     = rec.get(\"article\") or rec.get(\"text\")  # be tolerant\n",
    "\n",
    "            # locations may be saved under a few different keys; try them in order\n",
    "            loc_list = (\n",
    "                rec.get(\"locations_mentioned\")\n",
    "                or rec.get(\"_locations_structured\")\n",
    "                or rec.get(\"resolved_locations\")\n",
    "                or rec.get(\"locations\")\n",
    "                or []\n",
    "            )\n",
    "            if not isinstance(loc_list, list):\n",
    "                loc_list = [loc_list]\n",
    "\n",
    "            for loc in loc_list:\n",
    "                norm = parse_location(loc)\n",
    "                rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"source_file\": sf_name,    # basename only\n",
    "                    \"issue\": issue_name,       # derived issue name\n",
    "                    \"art_id\": art_id,\n",
    "                    \"article\": article,\n",
    "                    \"page\": page,\n",
    "                    # NEW enriched fields\n",
    "                    \"label\": norm[\"label\"],\n",
    "                    \"type\": norm[\"type\"],\n",
    "                    \"street\": norm[\"street\"],\n",
    "                    \"postcode\": norm[\"postcode\"],\n",
    "                    # Existing fields\n",
    "                    \"city\": norm[\"city\"],\n",
    "                    \"state\": norm[\"state\"],\n",
    "                    \"country\": norm[\"country\"],\n",
    "                    \"longitude\": norm[\"longitude\"],\n",
    "                    \"latitude\": norm[\"latitude\"],\n",
    "                    \"location_notes\": norm[\"location_notes\"],\n",
    "                })\n",
    "\n",
    "# ---- DataFrame + save\n",
    "loc_df = pd.DataFrame(rows)\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "loc_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(loc_df)} rows to {out_csv}\")\n",
    "\n",
    "loc_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
